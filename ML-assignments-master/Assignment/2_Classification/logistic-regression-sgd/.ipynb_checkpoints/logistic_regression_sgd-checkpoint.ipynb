{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/X_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-534377d8fd49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 第一行是feature的名称，所以先执行next(f)跳过第一行的内容；第一个dimension是id，feature[1:]从第二个dimension开始读取\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_fpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/X_train'"
     ]
    }
   ],
   "source": [
    "# 下载资料并做normalize，切为Training set和validation set\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x_train_fpath = '../data/X_train'\n",
    "y_train_fpath = '../data/Y_train'\n",
    "x_test_fpath  = '../data/X_test'\n",
    "\n",
    "# 第一行是feature的名称，所以先执行next(f)跳过第一行的内容；第一个dimension是id，feature[1:]从第二个dimension开始读取\n",
    "with open(x_train_fpath) as f:\n",
    "    next(f)\n",
    "    x_train = np.array([line.strip('\\n').split(',')[1:]  for line in f], dtype = float)\n",
    "\n",
    "with open(y_train_fpath) as f:\n",
    "    next(f)\n",
    "    y_train = np.array([line.strip('\\n').split(',')[1]  for line in f], dtype = float)\n",
    "    \n",
    "with open(x_test_fpath) as f:\n",
    "    next(f)\n",
    "    x_test = np.array([line.strip('\\n').split(',')[1:]   for line in f], dtype = float)\n",
    "    \n",
    "print('x_train :\\n',x_train,x_train.shape,'\\n')\n",
    "print('y_train :\\n',y_train,y_train.shape,'\\n')\n",
    "print('x_test :\\n',x_test,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(x, train = True, specified_column = None, x_mean = None, x_std = None):\n",
    "    '''\n",
    "    This function normalizes specific columns of x\n",
    "    注意，testing data要跟training data的normalize方式一致，要用training data的mean和std，\n",
    "    因此还需要input已知的x_mean和x_std\n",
    "    '''\n",
    "    # 如果没有指定列，那就穷举所有列，这里np.arange类似于range函数，只不过前者创造的对象是array类型\n",
    "    if specified_column == None:\n",
    "        specified_column = np.arange(x.shape[1])\n",
    "    \n",
    "    # train=True: for training data; train=False: for testing data，只计算training data的mean和std\n",
    "    if train:\n",
    "        # axis=0，对指定列求mean，注意np.mean返回的是一个列向量，因此需要用reshape(1,-1)转化成行向量\n",
    "        x_mean = np.mean(x[:, specified_column], axis = 0).reshape(1, -1)\n",
    "        # axis=0，对指定列求std\n",
    "        x_std  = np.std(x[:, specified_column], axis = 0).reshape(1, -1)\n",
    "     \n",
    "    # 对指定列进行normalize，注意相减的两个向量行数不同但列数相同，相当于前者的每一行都减去x_mean这个行向量，除法同理\n",
    "    # 分母加一个很小很小的数是为了避免标准差为0\n",
    "    x[:, specified_column] = (x[:, specified_column] - x_mean) / (x_std + 1e-8)\n",
    "    \n",
    "    return x, x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_split(x, y, validation_ratio = 0.25):\n",
    "    '''\n",
    "    This function splits data into training set and validation set\n",
    "    '''\n",
    "    train_size = int(len(x) * (1 - validation_ratio))\n",
    "    \n",
    "    #return x,y of training set and validation set  \n",
    "    # 如果返回值为x[:train_size, :]的话会报错，但这两种形式本质上是一样的，存疑\n",
    "    return x[:train_size], y[:train_size], x[train_size:], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize training data and testing data\n",
    "x_train, x_mean, x_std = _normalize(x_train, train = True)\n",
    "x_test, _, _ = _normalize(x_test, train = False, x_mean = x_mean, x_std = x_std)\n",
    "\n",
    "# split training data into training set and validation set\n",
    "x_training_set, y_training_set, x_validation_set, y_validation_set = _train_split(x_train, y_train, validation_ratio = 0.1)\n",
    "\n",
    "print('x_training_set : ', x_training_set.shape, '\\n', x_training_set)\n",
    "print('------------------------------------------------------------------------')\n",
    "print('y_training_set : ', y_training_set.shape, '\\n', y_training_set)\n",
    "print('------------------------------------------------------------------------')\n",
    "print('x_validation_set : ', x_validation_set.shape, '\\n', x_validation_set)\n",
    "print('------------------------------------------------------------------------')\n",
    "print('y_validation_set : ', y_validation_set.shape, '\\n', y_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful functions\n",
    "# np.dot()的作用主要体现在两个1-D向量相乘和一个N-D矩阵和一个1-D向量相乘的情景下：\n",
    "# 两个1-D向量A与B相乘(A、B元素数量必须相等)：等价于A、B对应元素相乘并累计求和，最终得到一个常量积；注意A*B和np.dot(A,B)的区别，前者是A、B对应元素相乘，每次相乘的积都作为新的1-D向量的一个元素，而不是把这些积累加为一个常量\n",
    "# 一个N-D矩阵A和一个1-D向量B相乘(A的每一行元素数量必须与B的元素数量相等)：等价于把这个N-D矩阵A拆成N个1-D向量，它们分别与B做1-D矩阵的相乘，得到的积作为结果的一个元素，总共有N个积，最终的结果就是由这N个积组成的1-D向量\n",
    "# np.dot()在w*x上的应用可以减少转置，在其他方面也有比较便利的应用\n",
    "def _shuffle(x, y):\n",
    "    '''\n",
    "    This function shuffles two equal-length list/array, x and y, together\n",
    "    '''\n",
    "    # 打乱原本的次序\n",
    "    randomize = np.arange(len(x))\n",
    "    np.random.shuffle(randomize)\n",
    "    \n",
    "    return x[randomize], y[randomize]\n",
    "\n",
    "def _sigmoid(z):\n",
    "    '''\n",
    "    sigmoid function can be used to calculate probability\n",
    "    To avoid overflow, minimum/maximum output value is set\n",
    "    '''\n",
    "    # np.clip(a, a_min, a_max)将数组a限制在a_min和a_max之间，超出范围的值将被赋以边界值\n",
    "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-6, 1 - (1e-6))\n",
    "\n",
    "def _f(x, w, b):\n",
    "    '''\n",
    "    logistic regression function, parameterized by w and b\n",
    "    \n",
    "    Arguements:\n",
    "        X: input data, shape = [batch_size, data_dimension]\n",
    "        w: weight vector, shape = [data_dimension, ]\n",
    "        b: bias, scalar\n",
    "    output:\n",
    "        predicted probability of each row of X being positively labeled, shape = [batch_size, ]\n",
    "    '''\n",
    "    # np.dot特别适合用来计算x*w，无需转置，直接就是N维x的每一行与一维w相乘得到的结果汇总成一个一维的y\n",
    "    return _sigmoid(np.dot(x, w) + b)\n",
    "\n",
    "def _predict(x, w, b):\n",
    "    '''\n",
    "    This function returns a truth value prediction for each row of x\n",
    "    by round function to make 0 or 1\n",
    "    '''\n",
    "    # 利用round函数的四舍五入功能把概率转化成0或1\n",
    "    return np.round(_f(x, w, b)).astype(np.int)\n",
    "    \n",
    "def _accuracy(y_predict, y_label):\n",
    "    '''\n",
    "    This function calculates prediction accuracy\n",
    "    '''\n",
    "    # 预测值和标签值相减，取绝对值后再求平均，相当于预测错误的个数(差为1)/总个数，即错误率，1-错误率即正确率\n",
    "    acc = 1 - np.mean(np.abs(y_predict - y_label))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function(cross_entropy) and gradient\n",
    "def _cross_entropy_loss(y_predict, y_label):\n",
    "    '''\n",
    "    This function computes the cross entropy \n",
    "    \n",
    "    Arguements:\n",
    "        y_pred: probabilistic predictions, float vector\n",
    "        Y_label: ground truth labels, bool vector\n",
    "    Output:\n",
    "        cross entropy, scalar    \n",
    "    '''\n",
    "    # cross_entropy = -Σ(y_head*ln(y)+(1-y_head)*ln(1-y))\n",
    "    # 注意，这里的np.dot可以直接计算两个一位矩阵的积(前提是元素个数一致)，无需转置；np.log()等价于ln()\n",
    "    # 因此这里的矩阵积实际上已经做了Σ的工作，左边的np.dot计算的是Σ(y_head*ln(y)，右边的np.dot计算的是Σ(1-y_head)*ln(1-y))\n",
    "    cross_entropy = -(np.dot(y_label, np.log(y_predict)) + np.dot((1 - y_label), np.log(1 - y_predict)))\n",
    "    \n",
    "    return cross_entropy\n",
    "\n",
    "def _gradient(x, y_label, w, b):\n",
    "    '''\n",
    "    This function computes the gradient of cross entropy loss with respect to weight w and bias b\n",
    "    loss function: -Σ (y_head*ln(y)+(1-y_head)*ln(1-y)), 分别对w和b求偏微分，可得\n",
    "    gradient of w: -Σ（y_head - y)*x\n",
    "    gradient of b: -Σ（y_head - y)\n",
    "    '''\n",
    "    y_predict = _f(x, w, b)\n",
    "    # 也可以是w_gradient = -np.dot(x.T, y_label - y_predict)\n",
    "    w_gradient = -np.sum((y_label - y_predict) * x.T, 1)\n",
    "    b_gradient = -np.sum(y_label - y_predict)\n",
    "    \n",
    "    return w_gradient, b_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch stochastic-gradient-descent\n",
    "\n",
    "train_size = x_training_set.shape[0]\n",
    "validation_size = x_validation_set.shape[0]\n",
    "dim = x_training_set.shape[1]\n",
    "# initialize w and b\n",
    "w = np.zeros(dim)\n",
    "b= np.zeros(1)\n",
    "\n",
    "# parameters for training\n",
    "max_iter = 100\n",
    "batch_size = 10\n",
    "learning_rate = 1\n",
    "\n",
    "# save the loss and accuracy\n",
    "training_set_loss = []\n",
    "training_set_acc = []\n",
    "validation_set_loss = []\n",
    "validation_set_acc = []\n",
    "\n",
    "w_adagrad = 1e-8\n",
    "b_adagrad = 1e-8\n",
    "\n",
    "# calcuate the number of parameter updates\n",
    "step = 1\n",
    "\n",
    "# training for iterations\n",
    "for epoch in range(max_iter):\n",
    "    # random shuffle at the beginning of each epoch\n",
    "    x_training_set, y_training_set = _shuffle(x_training_set, y_training_set)\n",
    "    \n",
    "    # mini-batch training\n",
    "    for i in range(int(np.floor(train_size / batch_size))):\n",
    "        # get the mini-batch\n",
    "        x = x_training_set[i * batch_size : (i + 1) * batch_size]\n",
    "        y = y_training_set[i * batch_size : (i + 1) * batch_size]\n",
    "        \n",
    "        # compute the gradient\n",
    "        w_gradient, b_gradient = _gradient(x, y, w, b)\n",
    "        \n",
    "        # compute the adagrad\n",
    "        w_adagrad = w_adagrad + np.power(w_gradient, 2)\n",
    "        b_adagrad = b_adagrad + np.power(b_gradient, 2)\n",
    "        \n",
    "        # gradient descent update \n",
    "        # learning rate decay with time\n",
    "        w = w - learning_rate * w_gradient / np.sqrt(w_adagrad)\n",
    "        b = b - learning_rate * b_gradient / np.sqrt(b_adagrad)\n",
    "        \n",
    "        step = step + 1\n",
    "        \n",
    "    # one epoch: compute loss and accuracy of training set and validation set\n",
    "    y_training_predict = _predict(x_training_set, w, b) # predict函数将Probability取round，只剩下0和1\n",
    "    y_probability = _f(x_training_set, w, b) # Probability用来计算cross_entropy，不能用round后的值，否则会出现ln(0)的错误\n",
    "    acc = _accuracy(y_training_predict, y_training_set)\n",
    "    loss = _cross_entropy_loss(y_probability, y_training_set) / train_size # average cross_entropy\n",
    "    training_set_acc.append(acc)\n",
    "    training_set_loss.append(loss)\n",
    "    print('training_set_acc_%d   : %f \\t training_set_loss_%d  : %f'%(epoch, acc, epoch, loss))\n",
    "    \n",
    "    y_validation_predict = _predict(x_validation_set, w, b)\n",
    "    y_probability = _f(x_validation_set, w, b)\n",
    "    acc = _accuracy(y_validation_predict, y_validation_set)\n",
    "    loss = _cross_entropy_loss(y_probability, y_validation_set) / validation_size # average cross_entropy\n",
    "    validation_set_acc.append(acc)\n",
    "    validation_set_loss.append(loss)\n",
    "    \n",
    "# validation_set的acc和loss只输出最后那一次\n",
    "print('validation_set_acc_%d : %f \\t validation_set_loss_%d : %f'%(epoch, acc, epoch, loss))\n",
    "print()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# loss curve\n",
    "plt.plot(training_set_loss)\n",
    "plt.plot(validation_set_loss)\n",
    "plt.title('Loss')\n",
    "plt.legend(['training_set', 'validation_set'])\n",
    "plt.savefig('loss_sgd.png')\n",
    "plt.show()\n",
    "\n",
    "# accuracy curve\n",
    "plt.plot(training_set_acc)\n",
    "plt.plot(validation_set_acc)\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['training_set', 'validation_set'])\n",
    "plt.savefig('acc_sgd.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch stochastic-gradient-descent\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "dim = x_train.shape[1]\n",
    "# initialize w and b\n",
    "w = np.zeros(dim)\n",
    "b= np.zeros(1)\n",
    "\n",
    "# parameters for training\n",
    "max_iter = 1000\n",
    "batch_size = 10\n",
    "learning_rate = 1\n",
    "\n",
    "# save the loss and accuracy\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "w_adagrad = 1e-8\n",
    "b_adagrad = 1e-8\n",
    "\n",
    "\n",
    "# training for iterations\n",
    "for epoch in range(max_iter):\n",
    "    # random shuffle at the beginning of each epoch\n",
    "    x_train, y_train = _shuffle(x_train, y_train)\n",
    "    \n",
    "    # mini-batch training\n",
    "    for i in range(int(np.floor(train_size / batch_size))):\n",
    "        # get the mini-batch\n",
    "        x = x_train[i * batch_size : (i + 1) * batch_size]\n",
    "        y = y_train[i * batch_size : (i + 1) * batch_size]\n",
    "        \n",
    "        # compute the gradient\n",
    "        w_gradient, b_gradient = _gradient(x, y, w, b)\n",
    "        \n",
    "        # compute the adagrad\n",
    "        w_adagrad = w_adagrad + np.power(w_gradient, 2)\n",
    "        b_adagrad = b_adagrad + np.power(b_gradient, 2)\n",
    "        \n",
    "        # gradient descent update \n",
    "        # learning rate decay with time\n",
    "        w = w - learning_rate * w_gradient / np.sqrt(w_adagrad)\n",
    "        b = b - learning_rate * b_gradient / np.sqrt(b_adagrad)\n",
    "    \n",
    "    # one epoch: compute loss and accuracy of training set and validation set\n",
    "    y_train_predict = _predict(x_train, w, b) # predict函数将Probability取round，只剩下0和1\n",
    "    y_probability = _f(x_train, w, b) # Probability用来计算cross_entropy，不能用round后的值，否则会出现ln(0)的错误\n",
    "    acc = _accuracy(y_train_predict, y_train)\n",
    "    loss = _cross_entropy_loss(y_probability, y_train) / train_size # average cross_entropy\n",
    "    train_acc.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    print('train_acc_%d   : %f \\t train_loss_%d  : %f'%(epoch, acc, epoch, loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss curve\n",
    "plt.plot(train_loss)\n",
    "plt.title('Loss')\n",
    "plt.legend(['train'])\n",
    "plt.show()\n",
    "\n",
    "# accuracy curve\n",
    "plt.plot(train_acc)\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['train'])\n",
    "plt.show()\n",
    "\n",
    "np.save('weight_adagrad_sgd.npy', w)\n",
    "np.save('bias_adagrad_sgd.npy', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict testing data\n",
    "import csv\n",
    "w = np.load('weight_adagrad_sgd.npy')\n",
    "b = np.load('bias_adagrad_sgd.npy')\n",
    "y_test_predict = _predict(x_test, w, b)\n",
    "print(y_test_predict, y_test_predict.shape)\n",
    "\n",
    "with open('predict_adagrad_sgd.csv', mode = 'w', newline = '') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    header = ['id', 'label']\n",
    "    print(header)\n",
    "    csv_writer.writerow(header)\n",
    "    for i in range(y_test_predict.shape[0]):\n",
    "        row = [str(i), y_test_predict[i]]\n",
    "        csv_writer.writerow(row)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('ml': conda)",
   "language": "python",
   "name": "python36864bitmlconda4727b916de1c4a8f8265036fed6d2bb8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
