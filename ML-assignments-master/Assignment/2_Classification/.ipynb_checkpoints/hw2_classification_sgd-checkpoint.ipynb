{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train :\n",
      " [[33.  1.  0. ... 52.  0.  1.]\n",
      " [63.  1.  0. ... 52.  0.  1.]\n",
      " [71.  0.  0. ...  0.  0.  1.]\n",
      " ...\n",
      " [16.  0.  0. ...  8.  1.  0.]\n",
      " [48.  1.  0. ... 52.  0.  1.]\n",
      " [48.  0.  0. ...  0.  0.  1.]] (54256, 510) \n",
      "\n",
      "y_train :\n",
      " [1. 0. 0. ... 0. 0. 0.] (54256,) \n",
      "\n",
      "x_test :\n",
      " [[37.  1.  0. ... 52.  0.  1.]\n",
      " [48.  1.  0. ... 52.  0.  1.]\n",
      " [68.  0.  0. ...  0.  1.  0.]\n",
      " ...\n",
      " [38.  1.  0. ... 52.  0.  1.]\n",
      " [17.  0.  0. ... 40.  1.  0.]\n",
      " [22.  0.  0. ... 25.  1.  0.]] (27622, 510)\n"
     ]
    }
   ],
   "source": [
    "# 下载资料并做normalize，切为Training set和validation set\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x_train_fpath = './data/X_train'\n",
    "y_train_fpath = './data/Y_train'\n",
    "x_test_fpath  = './data/X_test'\n",
    "output_fpath  = './output_{}.csv'\n",
    "\n",
    "# 第一行是feature的名称，所以先执行next(f)跳过第一行的内容；第一个dimension是id，feature[1:]从第二个dimension开始读取\n",
    "with open(x_train_fpath) as f:\n",
    "    next(f)\n",
    "    x_train = np.array([line.strip('\\n').split(',')[1:]  for line in f], dtype = float)\n",
    "\n",
    "with open(y_train_fpath) as f:\n",
    "    next(f)\n",
    "    y_train = np.array([line.strip('\\n').split(',')[1]  for line in f], dtype = float)\n",
    "    \n",
    "with open(x_test_fpath) as f:\n",
    "    next(f)\n",
    "    x_test = np.array([line.strip('\\n').split(',')[1:]   for line in f], dtype = float)\n",
    "    \n",
    "print('x_train :\\n',x_train,x_train.shape,'\\n')\n",
    "print('y_train :\\n',y_train,y_train.shape,'\\n')\n",
    "print('x_test :\\n',x_test,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(x, train = True, specified_column = None, x_mean = None, x_std = None):\n",
    "    '''\n",
    "    This function normalizes specific columns of x\n",
    "    注意，testing data要跟training data的normalize方式一致，要用training data的mean和std，\n",
    "    因此还需要input已知的x_mean和x_std\n",
    "    '''\n",
    "    # 如果没有指定列，那就穷举所有列，这里np.arange类似于range函数，只不过前者创造的对象是array类型\n",
    "    if specified_column == None:\n",
    "        specified_column = np.arange(x.shape[1])\n",
    "    \n",
    "    # train=True: for training data; train=False: for testing data，只计算training data的mean和std\n",
    "    if train:\n",
    "        # axis=0，对指定列求mean，注意np.mean返回的是一个列向量，因此需要用reshape(1,-1)转化成行向量\n",
    "        x_mean = np.mean(x[:, specified_column], axis = 0).reshape(1, -1)\n",
    "        # axis=0，对指定列求std\n",
    "        x_std  = np.std(x[:, specified_column], axis = 0).reshape(1, -1)\n",
    "     \n",
    "    # 对指定列进行normalize，注意相减的两个向量行数不同但列数相同，相当于前者的每一行都减去x_mean这个行向量，除法同理\n",
    "    # 分母加一个很小很小的数是为了避免标准差为0\n",
    "    x[:, specified_column] = (x[:, specified_column] - x_mean) / (x_std + 1e-8)\n",
    "    \n",
    "    return x, x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_split(x, y, validation_ratio = 0.25):\n",
    "    '''\n",
    "    This function splits data into training set and validation set\n",
    "    '''\n",
    "    train_size = int(len(x) * (1 - validation_ratio))\n",
    "    \n",
    "    #return x,y of training set and validation set  \n",
    "    # 如果返回值为x[:train_size, :]的话会报错，但这两种形式本质上是一样的，存疑\n",
    "    return x[:train_size], y[:train_size], x[train_size:], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_training_set :  (48830, 510) \n",
      " [[-0.42755297  0.99959458 -0.1822401  ...  0.80645987 -1.01485522\n",
      "   1.01485522]\n",
      " [ 1.19978056  0.99959458 -0.1822401  ...  0.80645987 -1.01485522\n",
      "   1.01485522]\n",
      " [ 1.63373617 -1.00040555 -0.1822401  ... -1.45536172 -1.01485522\n",
      "   1.01485522]\n",
      " ...\n",
      " [ 0.65733605 -1.00040555 -0.1822401  ...  0.80645987 -1.01485522\n",
      "   1.01485522]\n",
      " [ 0.27762489  0.99959458 -0.1822401  ...  0.28450104 -1.01485522\n",
      "   1.01485522]\n",
      " [ 0.16913599 -1.00040555 -0.1822401  ...  0.80645987  0.98536219\n",
      "  -0.98536219]]\n",
      "------------------------------------------------------------------------\n",
      "y_training_set :  (48830,) \n",
      " [1. 0. 0. ... 1. 0. 0.]\n",
      "------------------------------------------------------------------------\n",
      "x_validation_set :  (5426, 510) \n",
      " [[-0.48179742  0.99959458 -0.1822401  ...  0.80645987  0.98536219\n",
      "  -0.98536219]\n",
      " [-1.24121974 -1.00040555  5.48726602 ...  0.80645987 -1.01485522\n",
      "   1.01485522]\n",
      " [-0.04784181  0.99959458 -0.1822401  ...  0.80645987 -1.01485522\n",
      "   1.01485522]\n",
      " ...\n",
      " [-1.34970864 -1.00040555 -0.1822401  ... -1.10738916  0.98536219\n",
      "  -0.98536219]\n",
      " [ 0.3861138   0.99959458 -0.1822401  ...  0.80645987 -1.01485522\n",
      "   1.01485522]\n",
      " [ 0.3861138  -1.00040555 -0.1822401  ... -1.45536172 -1.01485522\n",
      "   1.01485522]]\n",
      "------------------------------------------------------------------------\n",
      "y_validation_set :  (5426,) \n",
      " [1. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# normalize training data and testing data\n",
    "x_train, x_mean, x_std = _normalize(x_train, train = True)\n",
    "x_test, _, _ = _normalize(x_test, train = False, x_mean = x_mean, x_std = x_std)\n",
    "\n",
    "# split training data into training set and validation set\n",
    "x_training_set, y_training_set, x_validation_set, y_validation_set = _train_split(x_train, y_train, validation_ratio = 0.1)\n",
    "\n",
    "print('x_training_set : ', x_training_set.shape, '\\n', x_training_set)\n",
    "print('------------------------------------------------------------------------')\n",
    "print('y_training_set : ', y_training_set.shape, '\\n', y_training_set)\n",
    "print('------------------------------------------------------------------------')\n",
    "print('x_validation_set : ', x_validation_set.shape, '\\n', x_validation_set)\n",
    "print('------------------------------------------------------------------------')\n",
    "print('y_validation_set : ', y_validation_set.shape, '\\n', y_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful functions\n",
    "# np.dot()的作用主要体现在两个1-D向量相乘和一个N-D矩阵和一个1-D向量相乘的情景下：\n",
    "# 两个1-D向量A与B相乘(A、B元素数量必须相等)：等价于A、B对应元素相乘并累计求和，最终得到一个常量积；注意A*B和np.dot(A,B)的区别，前者是A、B对应元素相乘，每次相乘的积都作为新的1-D向量的一个元素，而不是把这些积累加为一个常量\n",
    "# 一个N-D矩阵A和一个1-D向量B相乘(A的每一行元素数量必须与B的元素数量相等)：等价于把这个N-D矩阵A拆成N个1-D向量，它们分别与B做1-D矩阵的相乘，得到的积作为结果的一个元素，总共有N个积，最终的结果就是由这N个积组成的1-D向量\n",
    "# np.dot()在w*x上的应用可以减少转置，在其他方面也有比较便利的应用\n",
    "def _shuffle(x, y):\n",
    "    '''\n",
    "    This function shuffles two equal-length list/array, x and y, together\n",
    "    '''\n",
    "    # 打乱原本的次序\n",
    "    randomize = np.arange(len(x))\n",
    "    np.random.shuffle(randomize)\n",
    "    \n",
    "    return x[randomize], y[randomize]\n",
    "\n",
    "def _sigmoid(z):\n",
    "    '''\n",
    "    sigmoid function can be used to calculate probability\n",
    "    To avoid overflow, minimum/maximum output value is set\n",
    "    '''\n",
    "    # np.clip(a, a_min, a_max)将数组a限制在a_min和a_max之间，超出范围的值将被赋以边界值\n",
    "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-6, 1 - (1e-6))\n",
    "\n",
    "def _f(x, w, b):\n",
    "    '''\n",
    "    logistic regression function, parameterized by w and b\n",
    "    \n",
    "    Arguements:\n",
    "        X: input data, shape = [batch_size, data_dimension]\n",
    "        w: weight vector, shape = [data_dimension, ]\n",
    "        b: bias, scalar\n",
    "    output:\n",
    "        predicted probability of each row of X being positively labeled, shape = [batch_size, ]\n",
    "    '''\n",
    "    # np.dot特别适合用来计算x*w，无需转置，直接就是N维x的每一行与一维w相乘得到的结果汇总成一个一维的y\n",
    "    return _sigmoid(np.dot(x, w) + b)\n",
    "\n",
    "def _predict(x, w, b):\n",
    "    '''\n",
    "    This function returns a truth value prediction for each row of x\n",
    "    by round function to make 0 or 1\n",
    "    '''\n",
    "    # 利用round函数的四舍五入功能把概率转化成0或1\n",
    "    return np.round(_f(x, w, b)).astype(np.int)\n",
    "    \n",
    "def _accuracy(y_predict, y_label):\n",
    "    '''\n",
    "    This function calculates prediction accuracy\n",
    "    '''\n",
    "    # 预测值和标签值相减，取绝对值后再求平均，相当于预测错误的个数(差为1)/总个数，即错误率，1-错误率即正确率\n",
    "    acc = 1 - np.mean(np.abs(y_predict - y_label))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function(cross_entropy) and gradient\n",
    "def _cross_entropy_loss(y_predict, y_label):\n",
    "    '''\n",
    "    This function computes the cross entropy \n",
    "    \n",
    "    Arguements:\n",
    "        y_pred: probabilistic predictions, float vector\n",
    "        Y_label: ground truth labels, bool vector\n",
    "    Output:\n",
    "        cross entropy, scalar    \n",
    "    '''\n",
    "    # cross_entropy = -Σ(y_head*ln(y)+(1-y_head)*ln(1-y))\n",
    "    # 注意，这里的np.dot可以直接计算两个一位矩阵的积(前提是元素个数一致)，无需转置；np.log()等价于ln()\n",
    "    # 因此这里的矩阵积实际上已经做了Σ的工作，左边的np.dot计算的是Σ(y_head*ln(y)，右边的np.dot计算的是Σ(1-y_head)*ln(1-y))\n",
    "    cross_entropy = -(np.dot(y_label, np.log(y_predict)) + np.dot((1 - y_label), np.log(1 - y_predict)))\n",
    "    \n",
    "    return cross_entropy\n",
    "\n",
    "def _gradient(x, y_label, w, b):\n",
    "    '''\n",
    "    This function computes the gradient of cross entropy loss with respect to weight w and bias b\n",
    "    loss function: -Σ (y_head*ln(y)+(1-y_head)*ln(1-y)), 分别对w和b求偏微分，可得\n",
    "    gradient of w: -Σ（y_head - y)*x\n",
    "    gradient of b: -Σ（y_head - y)\n",
    "    '''\n",
    "    y_predict = _f(x, w, b)\n",
    "    # 也可以是w_gradient = -np.dot(x.T, y_label - y_predict)\n",
    "    w_gradient = -np.sum((y_label - y_predict) * x.T, 1)\n",
    "    b_gradient = -np.sum(y_label - y_predict)\n",
    "    \n",
    "    return w_gradient, b_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set_acc_0   : 0.853758 \t training_set_loss_0  : 1.231238\n",
      "validation_set_acc_0 : 0.842978 \t validation_set_loss_0 : 1.387043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set_acc_1   : 0.860557 \t training_set_loss_1  : 0.861367\n",
      "validation_set_acc_1 : 0.848139 \t validation_set_loss_1 : 0.978828\n",
      "\n",
      "training_set_acc_2   : 0.858919 \t training_set_loss_2  : 0.691638\n",
      "validation_set_acc_2 : 0.849982 \t validation_set_loss_2 : 0.772393\n",
      "\n",
      "training_set_acc_3   : 0.862216 \t training_set_loss_3  : 0.524193\n",
      "validation_set_acc_3 : 0.854036 \t validation_set_loss_3 : 0.589832\n",
      "\n",
      "training_set_acc_4   : 0.867827 \t training_set_loss_4  : 0.447262\n",
      "validation_set_acc_4 : 0.857538 \t validation_set_loss_4 : 0.508276\n",
      "\n",
      "training_set_acc_5   : 0.865574 \t training_set_loss_5  : 0.409718\n",
      "validation_set_acc_5 : 0.859749 \t validation_set_loss_5 : 0.463012\n",
      "\n",
      "training_set_acc_6   : 0.868093 \t training_set_loss_6  : 0.382282\n",
      "validation_set_acc_6 : 0.861408 \t validation_set_loss_6 : 0.426046\n",
      "\n",
      "training_set_acc_7   : 0.868913 \t training_set_loss_7  : 0.357397\n",
      "validation_set_acc_7 : 0.861408 \t validation_set_loss_7 : 0.394835\n",
      "\n",
      "training_set_acc_8   : 0.869097 \t training_set_loss_8  : 0.352555\n",
      "validation_set_acc_8 : 0.860671 \t validation_set_loss_8 : 0.382514\n",
      "\n",
      "training_set_acc_9   : 0.870879 \t training_set_loss_9  : 0.352337\n",
      "validation_set_acc_9 : 0.860302 \t validation_set_loss_9 : 0.379198\n",
      "\n",
      "training_set_acc_10   : 0.870858 \t training_set_loss_10  : 0.339295\n",
      "validation_set_acc_10 : 0.864725 \t validation_set_loss_10 : 0.361766\n",
      "\n",
      "training_set_acc_11   : 0.872128 \t training_set_loss_11  : 0.335619\n",
      "validation_set_acc_11 : 0.866015 \t validation_set_loss_11 : 0.349623\n",
      "\n",
      "training_set_acc_12   : 0.872783 \t training_set_loss_12  : 0.330857\n",
      "validation_set_acc_12 : 0.862882 \t validation_set_loss_12 : 0.353009\n",
      "\n",
      "training_set_acc_13   : 0.877289 \t training_set_loss_13  : 0.321833\n",
      "validation_set_acc_13 : 0.868411 \t validation_set_loss_13 : 0.347198\n",
      "\n",
      "training_set_acc_14   : 0.872353 \t training_set_loss_14  : 0.330259\n",
      "validation_set_acc_14 : 0.862514 \t validation_set_loss_14 : 0.355393\n",
      "\n",
      "training_set_acc_15   : 0.876818 \t training_set_loss_15  : 0.320810\n",
      "validation_set_acc_15 : 0.872466 \t validation_set_loss_15 : 0.339998\n",
      "\n",
      "training_set_acc_16   : 0.875179 \t training_set_loss_16  : 0.321960\n",
      "validation_set_acc_16 : 0.863804 \t validation_set_loss_16 : 0.346304\n",
      "\n",
      "training_set_acc_17   : 0.873561 \t training_set_loss_17  : 0.320462\n",
      "validation_set_acc_17 : 0.870439 \t validation_set_loss_17 : 0.339774\n",
      "\n",
      "training_set_acc_18   : 0.874933 \t training_set_loss_18  : 0.317534\n",
      "validation_set_acc_18 : 0.865831 \t validation_set_loss_18 : 0.340055\n",
      "\n",
      "training_set_acc_19   : 0.875691 \t training_set_loss_19  : 0.315467\n",
      "validation_set_acc_19 : 0.868043 \t validation_set_loss_19 : 0.332897\n",
      "\n",
      "training_set_acc_20   : 0.872271 \t training_set_loss_20  : 0.322728\n",
      "validation_set_acc_20 : 0.868964 \t validation_set_loss_20 : 0.348393\n",
      "\n",
      "training_set_acc_21   : 0.875732 \t training_set_loss_21  : 0.310006\n",
      "validation_set_acc_21 : 0.869886 \t validation_set_loss_21 : 0.330666\n",
      "\n",
      "training_set_acc_22   : 0.876551 \t training_set_loss_22  : 0.311072\n",
      "validation_set_acc_22 : 0.867858 \t validation_set_loss_22 : 0.330908\n",
      "\n",
      "training_set_acc_23   : 0.873172 \t training_set_loss_23  : 0.317190\n",
      "validation_set_acc_23 : 0.866753 \t validation_set_loss_23 : 0.336727\n",
      "\n",
      "training_set_acc_24   : 0.874626 \t training_set_loss_24  : 0.313951\n",
      "validation_set_acc_24 : 0.866015 \t validation_set_loss_24 : 0.343929\n",
      "\n",
      "training_set_acc_25   : 0.876326 \t training_set_loss_25  : 0.314501\n",
      "validation_set_acc_25 : 0.867674 \t validation_set_loss_25 : 0.333167\n",
      "\n",
      "training_set_acc_26   : 0.874155 \t training_set_loss_26  : 0.310956\n",
      "validation_set_acc_26 : 0.862882 \t validation_set_loss_26 : 0.336429\n",
      "\n",
      "training_set_acc_27   : 0.874340 \t training_set_loss_27  : 0.310840\n",
      "validation_set_acc_27 : 0.864910 \t validation_set_loss_27 : 0.334029\n",
      "\n",
      "training_set_acc_28   : 0.874421 \t training_set_loss_28  : 0.311569\n",
      "validation_set_acc_28 : 0.867121 \t validation_set_loss_28 : 0.340958\n",
      "\n",
      "training_set_acc_29   : 0.877575 \t training_set_loss_29  : 0.307543\n",
      "validation_set_acc_29 : 0.868964 \t validation_set_loss_29 : 0.327662\n",
      "\n",
      "training_set_acc_30   : 0.874299 \t training_set_loss_30  : 0.313319\n",
      "validation_set_acc_30 : 0.866200 \t validation_set_loss_30 : 0.338740\n",
      "\n",
      "training_set_acc_31   : 0.875650 \t training_set_loss_31  : 0.308382\n",
      "validation_set_acc_31 : 0.866200 \t validation_set_loss_31 : 0.335261\n",
      "\n",
      "training_set_acc_32   : 0.877698 \t training_set_loss_32  : 0.308144\n",
      "validation_set_acc_32 : 0.864357 \t validation_set_loss_32 : 0.334199\n",
      "\n",
      "training_set_acc_33   : 0.877801 \t training_set_loss_33  : 0.304490\n",
      "validation_set_acc_33 : 0.870254 \t validation_set_loss_33 : 0.324663\n",
      "\n",
      "training_set_acc_34   : 0.873377 \t training_set_loss_34  : 0.315479\n",
      "validation_set_acc_34 : 0.864910 \t validation_set_loss_34 : 0.337359\n",
      "\n",
      "training_set_acc_35   : 0.877801 \t training_set_loss_35  : 0.301414\n",
      "validation_set_acc_35 : 0.870623 \t validation_set_loss_35 : 0.325053\n",
      "\n",
      "training_set_acc_36   : 0.878333 \t training_set_loss_36  : 0.300945\n",
      "validation_set_acc_36 : 0.869149 \t validation_set_loss_36 : 0.325579\n",
      "\n",
      "training_set_acc_37   : 0.876736 \t training_set_loss_37  : 0.302937\n",
      "validation_set_acc_37 : 0.868780 \t validation_set_loss_37 : 0.327137\n",
      "\n",
      "training_set_acc_38   : 0.876469 \t training_set_loss_38  : 0.302614\n",
      "validation_set_acc_38 : 0.868411 \t validation_set_loss_38 : 0.319238\n",
      "\n",
      "training_set_acc_39   : 0.879603 \t training_set_loss_39  : 0.299558\n",
      "validation_set_acc_39 : 0.870623 \t validation_set_loss_39 : 0.325877\n",
      "\n",
      "training_set_acc_40   : 0.879418 \t training_set_loss_40  : 0.297321\n",
      "validation_set_acc_40 : 0.871176 \t validation_set_loss_40 : 0.321127\n",
      "\n",
      "training_set_acc_41   : 0.879214 \t training_set_loss_41  : 0.298842\n",
      "validation_set_acc_41 : 0.872650 \t validation_set_loss_41 : 0.321973\n",
      "\n",
      "training_set_acc_42   : 0.877104 \t training_set_loss_42  : 0.296634\n",
      "validation_set_acc_42 : 0.870807 \t validation_set_loss_42 : 0.323071\n",
      "\n",
      "training_set_acc_43   : 0.878702 \t training_set_loss_43  : 0.294612\n",
      "validation_set_acc_43 : 0.872097 \t validation_set_loss_43 : 0.318192\n",
      "\n",
      "training_set_acc_44   : 0.878784 \t training_set_loss_44  : 0.292573\n",
      "validation_set_acc_44 : 0.868964 \t validation_set_loss_44 : 0.318900\n",
      "\n",
      "training_set_acc_45   : 0.880422 \t training_set_loss_45  : 0.292648\n",
      "validation_set_acc_45 : 0.872835 \t validation_set_loss_45 : 0.313771\n",
      "\n",
      "training_set_acc_46   : 0.877043 \t training_set_loss_46  : 0.296448\n",
      "validation_set_acc_46 : 0.869701 \t validation_set_loss_46 : 0.321132\n",
      "\n",
      "training_set_acc_47   : 0.877104 \t training_set_loss_47  : 0.296012\n",
      "validation_set_acc_47 : 0.868043 \t validation_set_loss_47 : 0.323583\n",
      "\n",
      "training_set_acc_48   : 0.878108 \t training_set_loss_48  : 0.293418\n",
      "validation_set_acc_48 : 0.870807 \t validation_set_loss_48 : 0.317826\n",
      "\n",
      "training_set_acc_49   : 0.878886 \t training_set_loss_49  : 0.293346\n",
      "validation_set_acc_49 : 0.867490 \t validation_set_loss_49 : 0.318427\n",
      "\n",
      "training_set_acc_50   : 0.877719 \t training_set_loss_50  : 0.296154\n",
      "validation_set_acc_50 : 0.870070 \t validation_set_loss_50 : 0.319578\n",
      "\n",
      "training_set_acc_51   : 0.876428 \t training_set_loss_51  : 0.293879\n",
      "validation_set_acc_51 : 0.869149 \t validation_set_loss_51 : 0.313082\n",
      "\n",
      "training_set_acc_52   : 0.879193 \t training_set_loss_52  : 0.290318\n",
      "validation_set_acc_52 : 0.871729 \t validation_set_loss_52 : 0.313285\n",
      "\n",
      "training_set_acc_53   : 0.878435 \t training_set_loss_53  : 0.295124\n",
      "validation_set_acc_53 : 0.871729 \t validation_set_loss_53 : 0.319608\n",
      "\n",
      "training_set_acc_54   : 0.880197 \t training_set_loss_54  : 0.290408\n",
      "validation_set_acc_54 : 0.872282 \t validation_set_loss_54 : 0.315705\n",
      "\n",
      "training_set_acc_55   : 0.879111 \t training_set_loss_55  : 0.291285\n",
      "validation_set_acc_55 : 0.869886 \t validation_set_loss_55 : 0.315596\n",
      "\n",
      "training_set_acc_56   : 0.878435 \t training_set_loss_56  : 0.290847\n",
      "validation_set_acc_56 : 0.873940 \t validation_set_loss_56 : 0.311173\n",
      "\n",
      "training_set_acc_57   : 0.880401 \t training_set_loss_57  : 0.289726\n",
      "validation_set_acc_57 : 0.871913 \t validation_set_loss_57 : 0.313970\n",
      "\n",
      "training_set_acc_58   : 0.879541 \t training_set_loss_58  : 0.289117\n",
      "validation_set_acc_58 : 0.868964 \t validation_set_loss_58 : 0.319508\n",
      "\n",
      "training_set_acc_59   : 0.879705 \t training_set_loss_59  : 0.289216\n",
      "validation_set_acc_59 : 0.873572 \t validation_set_loss_59 : 0.312293\n",
      "\n",
      "training_set_acc_60   : 0.879357 \t training_set_loss_60  : 0.289616\n",
      "validation_set_acc_60 : 0.872097 \t validation_set_loss_60 : 0.313357\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set_acc_61   : 0.877268 \t training_set_loss_61  : 0.294581\n",
      "validation_set_acc_61 : 0.868227 \t validation_set_loss_61 : 0.319506\n",
      "\n",
      "training_set_acc_62   : 0.878804 \t training_set_loss_62  : 0.289641\n",
      "validation_set_acc_62 : 0.870254 \t validation_set_loss_62 : 0.317181\n",
      "\n",
      "training_set_acc_63   : 0.879296 \t training_set_loss_63  : 0.294147\n",
      "validation_set_acc_63 : 0.868227 \t validation_set_loss_63 : 0.318222\n",
      "\n",
      "training_set_acc_64   : 0.878353 \t training_set_loss_64  : 0.290707\n",
      "validation_set_acc_64 : 0.870254 \t validation_set_loss_64 : 0.313951\n",
      "\n",
      "training_set_acc_65   : 0.877022 \t training_set_loss_65  : 0.293039\n",
      "validation_set_acc_65 : 0.865463 \t validation_set_loss_65 : 0.316050\n",
      "\n",
      "training_set_acc_66   : 0.879541 \t training_set_loss_66  : 0.288369\n",
      "validation_set_acc_66 : 0.877258 \t validation_set_loss_66 : 0.310042\n",
      "\n",
      "training_set_acc_67   : 0.879603 \t training_set_loss_67  : 0.288513\n",
      "validation_set_acc_67 : 0.869517 \t validation_set_loss_67 : 0.320217\n",
      "\n",
      "training_set_acc_68   : 0.879029 \t training_set_loss_68  : 0.291191\n",
      "validation_set_acc_68 : 0.872097 \t validation_set_loss_68 : 0.309182\n",
      "\n",
      "training_set_acc_69   : 0.879214 \t training_set_loss_69  : 0.288231\n",
      "validation_set_acc_69 : 0.869886 \t validation_set_loss_69 : 0.313149\n",
      "\n",
      "training_set_acc_70   : 0.877760 \t training_set_loss_70  : 0.291290\n",
      "validation_set_acc_70 : 0.870807 \t validation_set_loss_70 : 0.316080\n",
      "\n",
      "training_set_acc_71   : 0.879336 \t training_set_loss_71  : 0.289327\n",
      "validation_set_acc_71 : 0.871913 \t validation_set_loss_71 : 0.314999\n",
      "\n",
      "training_set_acc_72   : 0.878026 \t training_set_loss_72  : 0.289807\n",
      "validation_set_acc_72 : 0.868780 \t validation_set_loss_72 : 0.318765\n",
      "\n",
      "training_set_acc_73   : 0.880033 \t training_set_loss_73  : 0.287443\n",
      "validation_set_acc_73 : 0.872282 \t validation_set_loss_73 : 0.311900\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-236ec94af041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# compute the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mw_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# compute the adagrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-35090816e9d7>\u001b[0m in \u001b[0;36m_gradient\u001b[0;34m(x, y_label, w, b)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# 也可以是w_gradient = -np.dot(x.T, y_label - y_predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mw_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_label\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mb_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_label\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2182\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     passkwargs = {k: v for k, v in kwargs.items()\n\u001b[0m\u001b[1;32m     75\u001b[0m                   if v is not np._NoValue}\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# mini-batch stochastic-gradient-descent\n",
    "\n",
    "train_size = x_training_set.shape[0]\n",
    "validation_size = x_validation_set.shape[0]\n",
    "dim = x_training_set.shape[1]\n",
    "# initialize w and b\n",
    "w = np.zeros(dim)\n",
    "b= np.zeros(1)\n",
    "\n",
    "# parameters for training\n",
    "max_iter = 100\n",
    "batch_size = 10\n",
    "learning_rate = 1\n",
    "\n",
    "# save the loss and accuracy\n",
    "training_set_loss = []\n",
    "training_set_acc = []\n",
    "validation_set_loss = []\n",
    "validation_set_acc = []\n",
    "\n",
    "w_adagrad = 1e-8\n",
    "b_adagrad = 1e-8\n",
    "\n",
    "# calcuate the number of parameter updates\n",
    "step = 1\n",
    "\n",
    "# training for iterations\n",
    "for epoch in range(max_iter):\n",
    "    # random shuffle at the beginning of each epoch\n",
    "    x_training_set, y_training_set = _shuffle(x_training_set, y_training_set)\n",
    "    \n",
    "    # mini-batch training\n",
    "    for i in range(int(np.floor(train_size / batch_size))):\n",
    "        # get the mini-batch\n",
    "        x = x_training_set[i * batch_size : (i + 1) * batch_size]\n",
    "        y = y_training_set[i * batch_size : (i + 1) * batch_size]\n",
    "        \n",
    "        # compute the gradient\n",
    "        w_gradient, b_gradient = _gradient(x, y, w, b)\n",
    "        \n",
    "        # compute the adagrad\n",
    "        w_adagrad = w_adagrad + np.power(w_gradient, 2)\n",
    "        b_adagrad = b_adagrad + np.power(b_gradient, 2)\n",
    "        \n",
    "        # gradient descent update \n",
    "        # learning rate decay with time\n",
    "        w = w - learning_rate * w_gradient / np.sqrt(w_adagrad)\n",
    "        b = b - learning_rate * b_gradient / np.sqrt(b_adagrad)\n",
    "        \n",
    "        step = step + 1\n",
    "        \n",
    "    # one epoch: compute loss and accuracy of training set and validation set\n",
    "    y_training_predict = _predict(x_training_set, w, b) # predict函数将Probability取round，只剩下0和1\n",
    "    y_probability = _f(x_training_set, w, b) # Probability用来计算cross_entropy，不能用round后的值，否则会出现ln(0)的错误\n",
    "    acc = _accuracy(y_training_predict, y_training_set)\n",
    "    loss = _cross_entropy_loss(y_probability, y_training_set) / train_size # average cross_entropy\n",
    "    training_set_acc.append(acc)\n",
    "    training_set_loss.append(loss)\n",
    "    print('training_set_acc_%d   : %f \\t training_set_loss_%d  : %f'%(epoch, acc, epoch, loss))\n",
    "    \n",
    "    y_validation_predict = _predict(x_validation_set, w, b)\n",
    "    y_probability = _f(x_validation_set, w, b)\n",
    "    acc = _accuracy(y_validation_predict, y_validation_set)\n",
    "    loss = _cross_entropy_loss(y_probability, y_validation_set) / validation_size # average cross_entropy\n",
    "    validation_set_acc.append(acc)\n",
    "    validation_set_loss.append(loss)\n",
    "    \n",
    "# validation_set的acc和loss只输出最后那一次\n",
    "print('validation_set_acc_%d : %f \\t validation_set_loss_%d : %f'%(epoch, acc, epoch, loss))\n",
    "print()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# loss curve\n",
    "plt.plot(training_set_loss)\n",
    "plt.plot(validation_set_loss)\n",
    "plt.title('Loss')\n",
    "plt.legend(['training_set', 'validation_set'])\n",
    "plt.savefig('loss_sgd.png')\n",
    "plt.show()\n",
    "\n",
    "# accuracy curve\n",
    "plt.plot(training_set_acc)\n",
    "plt.plot(validation_set_acc)\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['training_set', 'validation_set'])\n",
    "plt.savefig('acc_sgd.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch stochastic-gradient-descent\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "dim = x_train.shape[1]\n",
    "# initialize w and b\n",
    "w = np.zeros(dim)\n",
    "b= np.zeros(1)\n",
    "\n",
    "# parameters for training\n",
    "max_iter = 1000\n",
    "batch_size = 10\n",
    "learning_rate = 1\n",
    "\n",
    "# save the loss and accuracy\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "w_adagrad = 1e-8\n",
    "b_adagrad = 1e-8\n",
    "\n",
    "\n",
    "# training for iterations\n",
    "for epoch in range(max_iter):\n",
    "    # random shuffle at the beginning of each epoch\n",
    "    x_train, y_train = _shuffle(x_train, y_train)\n",
    "    \n",
    "    # mini-batch training\n",
    "    for i in range(int(np.floor(train_size / batch_size))):\n",
    "        # get the mini-batch\n",
    "        x = x_train[i * batch_size : (i + 1) * batch_size]\n",
    "        y = y_train[i * batch_size : (i + 1) * batch_size]\n",
    "        \n",
    "        # compute the gradient\n",
    "        w_gradient, b_gradient = _gradient(x, y, w, b)\n",
    "        \n",
    "        # compute the adagrad\n",
    "        w_adagrad = w_adagrad + np.power(w_gradient, 2)\n",
    "        b_adagrad = b_adagrad + np.power(b_gradient, 2)\n",
    "        \n",
    "        # gradient descent update \n",
    "        # learning rate decay with time\n",
    "        w = w - learning_rate * w_gradient / np.sqrt(w_adagrad)\n",
    "        b = b - learning_rate * b_gradient / np.sqrt(b_adagrad)\n",
    "    \n",
    "    # one epoch: compute loss and accuracy of training set and validation set\n",
    "    y_train_predict = _predict(x_train, w, b) # predict函数将Probability取round，只剩下0和1\n",
    "    y_probability = _f(x_train, w, b) # Probability用来计算cross_entropy，不能用round后的值，否则会出现ln(0)的错误\n",
    "    acc = _accuracy(y_train_predict, y_train)\n",
    "    loss = _cross_entropy_loss(y_probability, y_train) / train_size # average cross_entropy\n",
    "    train_acc.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    print('train_acc_%d   : %f \\t train_loss_%d  : %f'%(epoch, acc, epoch, loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss curve\n",
    "plt.plot(train_loss)\n",
    "plt.title('Loss')\n",
    "plt.legend(['train'])\n",
    "plt.show()\n",
    "\n",
    "# accuracy curve\n",
    "plt.plot(train_acc)\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['train'])\n",
    "plt.show()\n",
    "\n",
    "np.save('weight_adagrad_sgd.npy', w)\n",
    "np.save('bias_adagrad_sgd.npy', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict testing data\n",
    "import csv\n",
    "w = np.load('weight_adagrad_sgd.npy')\n",
    "b = np.load('bias_adagrad_sgd.npy')\n",
    "y_test_predict = _predict(x_test, w, b)\n",
    "print(y_test_predict, y_test_predict.shape)\n",
    "\n",
    "with open('predict_adagrad_sgd.csv', mode = 'w', newline = '') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    header = ['id', 'label']\n",
    "    print(header)\n",
    "    csv_writer.writerow(header)\n",
    "    for i in range(y_test_predict.shape[0]):\n",
    "        row = [str(i), y_test_predict[i]]\n",
    "        csv_writer.writerow(row)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('ml': conda)",
   "language": "python",
   "name": "python36864bitmlconda4727b916de1c4a8f8265036fed6d2bb8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
