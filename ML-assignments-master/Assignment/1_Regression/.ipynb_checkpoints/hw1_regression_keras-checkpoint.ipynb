{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>日期</th>\n",
       "      <th>測站</th>\n",
       "      <th>測項</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>AMB_TEMP</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>CH4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>CO</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>NMHC</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>NO</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>THC</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>WD_HR</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>44</td>\n",
       "      <td>55</td>\n",
       "      <td>68</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>308</td>\n",
       "      <td>327</td>\n",
       "      <td>21</td>\n",
       "      <td>100</td>\n",
       "      <td>109</td>\n",
       "      <td>108</td>\n",
       "      <td>114</td>\n",
       "      <td>108</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>WIND_DIREC</td>\n",
       "      <td>36</td>\n",
       "      <td>55</td>\n",
       "      <td>72</td>\n",
       "      <td>327</td>\n",
       "      <td>74</td>\n",
       "      <td>52</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>311</td>\n",
       "      <td>52</td>\n",
       "      <td>54</td>\n",
       "      <td>121</td>\n",
       "      <td>97</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>100</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>WIND_SPEED</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>WS_HR</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4320 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              日期  測站          測項     0     1     2     3     4     5     6  \\\n",
       "0       2014/1/1  豐原    AMB_TEMP    14    14    14    13    12    12    12   \n",
       "1       2014/1/1  豐原         CH4   1.8   1.8   1.8   1.8   1.8   1.8   1.8   \n",
       "2       2014/1/1  豐原          CO  0.51  0.41  0.39  0.37  0.35   0.3  0.37   \n",
       "3       2014/1/1  豐原        NMHC   0.2  0.15  0.13  0.12  0.11  0.06   0.1   \n",
       "4       2014/1/1  豐原          NO   0.9   0.6   0.5   1.7   1.8   1.5   1.9   \n",
       "...          ...  ..         ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "4315  2014/12/20  豐原         THC   1.8   1.8   1.8   1.8   1.8   1.7   1.7   \n",
       "4316  2014/12/20  豐原       WD_HR    46    13    61    44    55    68    66   \n",
       "4317  2014/12/20  豐原  WIND_DIREC    36    55    72   327    74    52    59   \n",
       "4318  2014/12/20  豐原  WIND_SPEED   1.9   2.4   1.9   2.8   2.3   1.9   2.1   \n",
       "4319  2014/12/20  豐原       WS_HR   0.7   0.8   1.8     1   1.9   1.7   2.1   \n",
       "\n",
       "      ...    14    15    16    17    18    19    20    21    22    23  \n",
       "0     ...    22    22    21    19    17    16    15    15    15    15  \n",
       "1     ...   1.8   1.8   1.8   1.8   1.8   1.8   1.8   1.8   1.8   1.8  \n",
       "2     ...  0.37  0.37  0.47  0.69  0.56  0.45  0.38  0.35  0.36  0.32  \n",
       "3     ...   0.1  0.13  0.14  0.23  0.18  0.12   0.1  0.09   0.1  0.08  \n",
       "4     ...   2.5   2.2   2.5   2.3   2.1   1.9   1.5   1.6   1.8   1.5  \n",
       "...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "4315  ...   1.8   1.8     2   2.1     2   1.9   1.9   1.9     2     2  \n",
       "4316  ...    59   308   327    21   100   109   108   114   108   109  \n",
       "4317  ...    18   311    52    54   121    97   107   118   100   105  \n",
       "4318  ...   2.3   2.6   1.3     1   1.5     1   1.7   1.5     2     2  \n",
       "4319  ...   1.3   1.7   0.7   0.4   1.1   1.4   1.3   1.6   1.8     2  \n",
       "\n",
       "[4320 rows x 27 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data=pd.read_csv('./train.csv',encoding='big5')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['14', '14', '14', ..., '15', '15', '15'],\n",
       "       ['1.8', '1.8', '1.8', ..., '1.8', '1.8', '1.8'],\n",
       "       ['0.51', '0.41', '0.39', ..., '0.35', '0.36', '0.32'],\n",
       "       ...,\n",
       "       ['36', '55', '72', ..., '118', '100', '105'],\n",
       "       ['1.9', '2.4', '1.9', ..., '1.5', '2', '2'],\n",
       "       ['0.7', '0.8', '1.8', ..., '1.6', '1.8', '2']], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们只需要第三列以后的data\n",
    "data = data.iloc[:,3:]\n",
    "# 将NR的数据全部置为0\n",
    "data[data == 'NR'] = 0\n",
    "# 将dataframe幻化成numpy\n",
    "raw_data = data.to_numpy()\n",
    "\n",
    "#查看raw_data\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.  , 14.  , 14.  , ..., 14.  , 13.  , 13.  ],\n",
       "       [ 1.8 ,  1.8 ,  1.8 , ...,  1.8 ,  1.8 ,  1.8 ],\n",
       "       [ 0.51,  0.41,  0.39, ...,  0.34,  0.41,  0.43],\n",
       "       ...,\n",
       "       [35.  , 79.  ,  2.4 , ..., 48.  , 63.  , 53.  ],\n",
       "       [ 1.4 ,  1.8 ,  1.  , ...,  1.1 ,  1.9 ,  1.9 ],\n",
       "       [ 0.5 ,  0.9 ,  0.6 , ...,  1.2 ,  1.2 ,  1.3 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在要开始处理Training data\n",
    "# 由于提供的数据是每个月前20天(每天24小时)的资料(共18种类型)，因此每个月的这20*24=480个小时的时间是连续的，每10个小时可以作为一笔data，480小时共471笔data\n",
    "# 由于不同月份之间的时间是不连续的，因此先把所有的资料按照月份分开\n",
    "month_data = {}\n",
    "for month in range(12):\n",
    "  # 每个月共20*24=480份数据，共18种空气成分，每一种成分每月都有480份数据，因此初始化一个18*480的array\n",
    "  temp_data = np.empty([18,480])\n",
    "  for day in range(20):\n",
    "    # temp_data中加入第day天的数据，由于每天都有24份数据共24列,故列的范围是24*day~24*(day+1)；选择加入temp_data的是第20×month+day这一天的数据，由于每天有18种大气成分的数据共18行，因此行的范围是18*(20*month+day)~18*(20*month+day+1)\n",
    "    temp_data[:, 24 * day : 24 * (day + 1)] = raw_data[18 * (20 * month + day) : 18 * (20 * month + day + 1)]\n",
    "  # 把这个月的data放进month_data[month]里去\n",
    "  month_data[month]=temp_data\n",
    "\n",
    "# 查看第一个月的数据，并借此查看数据的结构：每一行都是一种大气成分在24*20个连续小时内的值\n",
    "month_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.  14.  14.  ...  2.   2.   0.5]\n",
      " [14.  14.  13.  ...  2.   0.5  0.3]\n",
      " [14.  13.  12.  ...  0.5  0.3  0.8]\n",
      " ...\n",
      " [17.  18.  19.  ...  1.1  1.4  1.3]\n",
      " [18.  19.  18.  ...  1.4  1.3  1.6]\n",
      " [19.  18.  17.  ...  1.3  1.6  1.8]]\n",
      "[[30.]\n",
      " [41.]\n",
      " [44.]\n",
      " ...\n",
      " [17.]\n",
      " [24.]\n",
      " [29.]]\n"
     ]
    }
   ],
   "source": [
    "# 这个时候我们就要在12段连续的时间里每10个小时取出一笔Training data\n",
    "# 每个月连续时间有20*24=480h，每10h作为一笔data，共可以分为471笔data；一共12个月，因此共12*471笔data\n",
    "# 每一笔data，前9h为input，最后1h的PM2.5为output，因此input是一个18*9的矩阵，如果把它摊平就是一个18*9的feature vector\n",
    "\n",
    "# x是input，共12*471笔data，每个input是一个18*9的feature vector\n",
    "x = np.empty([12 * 471, 18 * 9], dtype = float)\n",
    "# y是output，共12*471笔data，每个output是第10个小时的第9种大气成分PM2.5，因此只有1维\n",
    "y = np.empty([12 * 471, 1], dtype = float)\n",
    "\n",
    "# 取出input和output\n",
    "for month in range(12):\n",
    "  # 每个月份共471笔data，依次遍历即可\n",
    "  for i in range(471):\n",
    "    # 用一个18行(18种物质),10列(10个小时的数据，包括input和output)的temp_data来存放每一笔Training data\n",
    "    temp_data = np.empty([18, 10])\n",
    "    # 第i笔data的范围是i~i+9，共10列，注意这里i+10是开区间，实际只取到了i+9\n",
    "    temp_data = month_data[month][:, i : i + 10]\n",
    "    # 将temp_data前9列的作为input的数据(18行9列)用reshape平摊到一个18*9的行向量上\n",
    "    x[471 * month + i, :] = temp_data[:, 0 : 9].reshape(1,-1)\n",
    "    # temp_data的第10列作为output，只有第10行的PM2.5值是有用的\n",
    "    y[471 * month + i] = temp_data[9, 9]\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35825331, -1.35883937, -1.359222  , ...,  0.26650729,\n",
       "         0.2656797 , -1.14082131],\n",
       "       [-1.35825331, -1.35883937, -1.51819928, ...,  0.26650729,\n",
       "        -1.13963133, -1.32832904],\n",
       "       [-1.35825331, -1.51789368, -1.67717656, ..., -1.13923451,\n",
       "        -1.32700613, -0.85955971],\n",
       "       ...,\n",
       "       [-0.88092053, -0.72262212, -0.56433559, ..., -0.57693779,\n",
       "        -0.29644471, -0.39079039],\n",
       "       [-0.7218096 , -0.56356781, -0.72331287, ..., -0.29578943,\n",
       "        -0.39013211, -0.1095288 ],\n",
       "       [-0.56269867, -0.72262212, -0.88229015, ..., -0.38950555,\n",
       "        -0.10906991,  0.07797893]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个时候我们已经有了Training data的x和y，但由于feature的每一个dimension大小范围都是不一样的，因此还要对其做feature scale\n",
    "# 求出期望mean和标准差std，利用公式x'=(x-u)/σ来使同一列的数据同时满足同一个分布\n",
    "\n",
    "# 直接调用numpy的mean和std计算期望和标准差，axis=0表示对列进行计算\n",
    "mean_x = np.mean(x, axis = 0)\n",
    "std_x = np.std(x, axis = 0)\n",
    "\n",
    "# 对每一个feature都进行normalize归一化处理\n",
    "for i in range(len(x)):\n",
    "  for j in range(len(x[0])):\n",
    "    if std_x[j] != 0:\n",
    "      x[i][j] = (x[i][j] - mean_x[j])/std_x[j]\n",
    "\n",
    "# 查看normalize后的input x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_set len: 4521\n",
      "\n",
      "[[-1.35825331 -1.35883937 -1.359222   ...  0.26650729  0.2656797\n",
      "  -1.14082131]\n",
      " [-1.35825331 -1.35883937 -1.51819928 ...  0.26650729 -1.13963133\n",
      "  -1.32832904]\n",
      " [-1.35825331 -1.51789368 -1.67717656 ... -1.13923451 -1.32700613\n",
      "  -0.85955971]\n",
      " ...\n",
      " [ 0.86929969  0.70886668  0.38952809 ...  1.39110073  0.2656797\n",
      "  -0.39079039]\n",
      " [ 0.71018876  0.39075806  0.07157353 ...  0.26650729 -0.39013211\n",
      "  -0.39079039]\n",
      " [ 0.3919669   0.07264944  0.07157353 ... -0.38950555 -0.39013211\n",
      "  -0.85955971]]\n",
      "---------------------------------------------------------------------------\n",
      "y_train_set len: 4521\n",
      "\n",
      "[[30.]\n",
      " [41.]\n",
      " [44.]\n",
      " ...\n",
      " [ 7.]\n",
      " [ 5.]\n",
      " [14.]]\n",
      "---------------------------------------------------------------------------\n",
      "x_validation_set len: 1131\n",
      "\n",
      "[[ 0.07374504  0.07264944  0.07157353 ... -0.38950555 -0.85856912\n",
      "  -0.57829812]\n",
      " [ 0.07374504  0.07264944  0.23055081 ... -0.85808615 -0.57750692\n",
      "   0.54674825]\n",
      " [ 0.07374504  0.23170375  0.23055081 ... -0.57693779  0.54674191\n",
      "  -0.1095288 ]\n",
      " ...\n",
      " [-0.88092053 -0.72262212 -0.56433559 ... -0.57693779 -0.29644471\n",
      "  -0.39079039]\n",
      " [-0.7218096  -0.56356781 -0.72331287 ... -0.29578943 -0.39013211\n",
      "  -0.1095288 ]\n",
      " [-0.56269867 -0.72262212 -0.88229015 ... -0.38950555 -0.10906991\n",
      "   0.07797893]]\n",
      "---------------------------------------------------------------------------\n",
      "y_validation_set len: 1131\n",
      "\n",
      "[[13.]\n",
      " [24.]\n",
      " [22.]\n",
      " ...\n",
      " [17.]\n",
      " [24.]\n",
      " [29.]]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 为了有效衡量testing data的bias对结果的影响，这里切出一块validaiton set\n",
    "import math\n",
    "x_train_set = x[: math.floor(len(x) * 0.8), :]\n",
    "y_train_set = y[: math.floor(len(y) * 0.8), :]\n",
    "x_validation_set = x[math.floor(len(x) * 0.8): , :]\n",
    "y_validation_set = y[math.floor(len(y) * 0.8): , :]\n",
    "\n",
    "print('x_train_set len: ' + str(len(x_train_set)) + '\\n')\n",
    "print(x_train_set)\n",
    "print('---------------------------------------------------------------------------')\n",
    "print('y_train_set len: ' + str(len(y_train_set)) + '\\n')\n",
    "print(y_train_set)\n",
    "print('---------------------------------------------------------------------------')\n",
    "print('x_validation_set len: ' + str(len(x_validation_set)) + '\\n')\n",
    "print(x_validation_set)\n",
    "print('---------------------------------------------------------------------------')\n",
    "print('y_validation_set len: ' + str(len(y_validation_set)) + '\\n')\n",
    "print(y_validation_set)\n",
    "print('---------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24447681, -0.24545919, -0.40535831, ..., -0.67065391,\n",
       "        -1.04594393,  0.07797893],\n",
       "       [-1.35825331, -1.51789368, -1.51819928, ...,  0.17279117,\n",
       "        -0.10906991, -0.48454426],\n",
       "       [ 1.5057434 ,  1.34508393,  1.50236906, ..., -1.32666675,\n",
       "        -1.04594393, -0.57829812],\n",
       "       ...,\n",
       "       [ 0.3919669 ,  0.54981237,  0.70748265, ...,  0.26650729,\n",
       "        -0.20275731,  1.20302531],\n",
       "       [-1.8355861 , -1.8360023 , -1.83615384, ..., -1.04551839,\n",
       "        -1.13963133, -1.14082131],\n",
       "       [-1.35825331, -1.35883937, -1.359222  , ...,  2.98427476,\n",
       "         3.26367657,  1.76554849]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 处理数据,主要是testing data\n",
    "# 此时的x不需要再用np.concatenate函数来拼接一个常数项的feature，no need:x = np.concatenate((np.ones([12 * 471, 1]), x), axis = 1).astype(float)\n",
    "\n",
    "testdata = pd.read_csv('./test.csv', header = None, encoding = 'big5')\n",
    "test_data = testdata.iloc[:, 2:]\n",
    "test_data[test_data == 'NR'] = 0\n",
    "test_data = test_data.to_numpy()\n",
    "test_x = np.empty([240, 18*9], dtype = float)\n",
    "for i in range(240):\n",
    "    test_x[i, :] = test_data[18 * i: 18* (i + 1), :].reshape(1, -1)\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(test_x[0])):\n",
    "        if std_x[j] != 0:\n",
    "            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]\n",
    "# no need: test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 50)                8150      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 10,751\n",
      "Trainable params: 10,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "4521/4521 [==============================] - 3s 723us/step - loss: 712.6456\n",
      "Epoch 2/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 532.1202\n",
      "Epoch 3/200\n",
      "4521/4521 [==============================] - 0s 52us/step - loss: 315.3507\n",
      "Epoch 4/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 219.6304\n",
      "Epoch 5/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 198.6221\n",
      "Epoch 6/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 182.8380\n",
      "Epoch 7/200\n",
      "4521/4521 [==============================] - 0s 69us/step - loss: 162.8655\n",
      "Epoch 8/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 151.5731\n",
      "Epoch 9/200\n",
      "4521/4521 [==============================] - 0s 65us/step - loss: 144.8041\n",
      "Epoch 10/200\n",
      "4521/4521 [==============================] - 0s 72us/step - loss: 139.0187\n",
      "Epoch 11/200\n",
      "4521/4521 [==============================] - 0s 73us/step - loss: 139.4781\n",
      "Epoch 12/200\n",
      "4521/4521 [==============================] - 0s 53us/step - loss: 127.3880\n",
      "Epoch 13/200\n",
      "4521/4521 [==============================] - 0s 52us/step - loss: 119.8958\n",
      "Epoch 14/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 117.0175\n",
      "Epoch 15/200\n",
      "4521/4521 [==============================] - 0s 64us/step - loss: 113.7257\n",
      "Epoch 16/200\n",
      "4521/4521 [==============================] - 0s 52us/step - loss: 110.4811\n",
      "Epoch 17/200\n",
      "4521/4521 [==============================] - 0s 65us/step - loss: 106.1514\n",
      "Epoch 18/200\n",
      "4521/4521 [==============================] - 0s 61us/step - loss: 100.6395\n",
      "Epoch 19/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 107.2131\n",
      "Epoch 20/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 100.3382\n",
      "Epoch 21/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 99.3555\n",
      "Epoch 22/200\n",
      "4521/4521 [==============================] - 0s 55us/step - loss: 100.3421\n",
      "Epoch 23/200\n",
      "4521/4521 [==============================] - 0s 53us/step - loss: 93.5044\n",
      "Epoch 24/200\n",
      "4521/4521 [==============================] - 0s 72us/step - loss: 97.3427\n",
      "Epoch 25/200\n",
      "4521/4521 [==============================] - 0s 78us/step - loss: 92.2747\n",
      "Epoch 26/200\n",
      "4521/4521 [==============================] - 0s 70us/step - loss: 93.2305\n",
      "Epoch 27/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 93.6170\n",
      "Epoch 28/200\n",
      "4521/4521 [==============================] - 0s 55us/step - loss: 87.1190\n",
      "Epoch 29/200\n",
      "4521/4521 [==============================] - 0s 72us/step - loss: 88.7100\n",
      "Epoch 30/200\n",
      "4521/4521 [==============================] - 0s 70us/step - loss: 81.7653\n",
      "Epoch 31/200\n",
      "4521/4521 [==============================] - 0s 68us/step - loss: 84.3868\n",
      "Epoch 32/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 85.2062\n",
      "Epoch 33/200\n",
      "4521/4521 [==============================] - 0s 108us/step - loss: 84.5379\n",
      "Epoch 34/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 84.0709\n",
      "Epoch 35/200\n",
      "4521/4521 [==============================] - 0s 68us/step - loss: 79.3452\n",
      "Epoch 36/200\n",
      "4521/4521 [==============================] - 0s 65us/step - loss: 82.0936\n",
      "Epoch 37/200\n",
      "4521/4521 [==============================] - 0s 72us/step - loss: 77.3694\n",
      "Epoch 38/200\n",
      "4521/4521 [==============================] - 0s 63us/step - loss: 81.2476\n",
      "Epoch 39/200\n",
      "4521/4521 [==============================] - 0s 55us/step - loss: 82.7315\n",
      "Epoch 40/200\n",
      "4521/4521 [==============================] - 0s 67us/step - loss: 80.0192\n",
      "Epoch 41/200\n",
      "4521/4521 [==============================] - 0s 76us/step - loss: 78.1594\n",
      "Epoch 42/200\n",
      "4521/4521 [==============================] - 0s 63us/step - loss: 77.0605\n",
      "Epoch 43/200\n",
      "4521/4521 [==============================] - 0s 51us/step - loss: 76.5803\n",
      "Epoch 44/200\n",
      "4521/4521 [==============================] - 0s 65us/step - loss: 73.8800\n",
      "Epoch 45/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 75.7828\n",
      "Epoch 46/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 73.5504\n",
      "Epoch 47/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 76.0626\n",
      "Epoch 48/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 72.4565\n",
      "Epoch 49/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 73.1130\n",
      "Epoch 50/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 74.4996\n",
      "Epoch 51/200\n",
      "4521/4521 [==============================] - 0s 52us/step - loss: 71.9158\n",
      "Epoch 52/200\n",
      "4521/4521 [==============================] - 0s 79us/step - loss: 73.5758\n",
      "Epoch 53/200\n",
      "4521/4521 [==============================] - 0s 68us/step - loss: 73.7181\n",
      "Epoch 54/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 72.0529\n",
      "Epoch 55/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 72.3304\n",
      "Epoch 56/200\n",
      "4521/4521 [==============================] - 0s 68us/step - loss: 73.1029\n",
      "Epoch 57/200\n",
      "4521/4521 [==============================] - 0s 68us/step - loss: 71.5593\n",
      "Epoch 58/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 69.6419\n",
      "Epoch 59/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 70.2859\n",
      "Epoch 60/200\n",
      "4521/4521 [==============================] - 0s 64us/step - loss: 71.3478\n",
      "Epoch 61/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 68.4082\n",
      "Epoch 62/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 66.6244\n",
      "Epoch 63/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 68.1854\n",
      "Epoch 64/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 68.8732\n",
      "Epoch 65/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 65.5051\n",
      "Epoch 66/200\n",
      "4521/4521 [==============================] - 0s 63us/step - loss: 68.7841\n",
      "Epoch 67/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 66.2123\n",
      "Epoch 68/200\n",
      "4521/4521 [==============================] - 0s 94us/step - loss: 70.1418\n",
      "Epoch 69/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 65.8911\n",
      "Epoch 70/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 66.4944\n",
      "Epoch 71/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 63.4792\n",
      "Epoch 72/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 64.9091\n",
      "Epoch 73/200\n",
      "4521/4521 [==============================] - 0s 69us/step - loss: 68.1923\n",
      "Epoch 74/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 64.8364\n",
      "Epoch 75/200\n",
      "4521/4521 [==============================] - 0s 71us/step - loss: 65.6137\n",
      "Epoch 76/200\n",
      "4521/4521 [==============================] - 0s 73us/step - loss: 64.8076\n",
      "Epoch 77/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 63.4097\n",
      "Epoch 78/200\n",
      "4521/4521 [==============================] - 0s 53us/step - loss: 66.9160\n",
      "Epoch 79/200\n",
      "4521/4521 [==============================] - 0s 55us/step - loss: 66.5283\n",
      "Epoch 80/200\n",
      "4521/4521 [==============================] - 0s 70us/step - loss: 61.9621\n",
      "Epoch 81/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 62.9065\n",
      "Epoch 82/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 64.7629\n",
      "Epoch 83/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 61.8677\n",
      "Epoch 84/200\n",
      "4521/4521 [==============================] - 0s 51us/step - loss: 62.5490\n",
      "Epoch 85/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 60.1789\n",
      "Epoch 86/200\n",
      "4521/4521 [==============================] - 0s 52us/step - loss: 62.8663\n",
      "Epoch 87/200\n",
      "4521/4521 [==============================] - 0s 61us/step - loss: 62.9233\n",
      "Epoch 88/200\n",
      "4521/4521 [==============================] - 0s 93us/step - loss: 61.4635\n",
      "Epoch 89/200\n",
      "4521/4521 [==============================] - 0s 67us/step - loss: 60.2847\n",
      "Epoch 90/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 66.7416\n",
      "Epoch 91/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 62.1827\n",
      "Epoch 92/200\n",
      "4521/4521 [==============================] - 0s 68us/step - loss: 64.0299\n",
      "Epoch 93/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 61.4686\n",
      "Epoch 94/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 61.5257\n",
      "Epoch 95/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 62.1562\n",
      "Epoch 96/200\n",
      "4521/4521 [==============================] - 0s 65us/step - loss: 59.8917\n",
      "Epoch 97/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 61.1441\n",
      "Epoch 98/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 64.4722\n",
      "Epoch 99/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 60.5953\n",
      "Epoch 100/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 58.1298\n",
      "Epoch 101/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 58.9759\n",
      "Epoch 102/200\n",
      "4521/4521 [==============================] - 0s 55us/step - loss: 58.9387\n",
      "Epoch 103/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 62.1017\n",
      "Epoch 104/200\n",
      "4521/4521 [==============================] - 0s 87us/step - loss: 60.0094\n",
      "Epoch 105/200\n",
      "4521/4521 [==============================] - 0s 53us/step - loss: 59.7874\n",
      "Epoch 106/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 57.8966\n",
      "Epoch 107/200\n",
      "4521/4521 [==============================] - 0s 67us/step - loss: 58.2583\n",
      "Epoch 108/200\n",
      "4521/4521 [==============================] - 0s 61us/step - loss: 57.8462\n",
      "Epoch 109/200\n",
      "4521/4521 [==============================] - 0s 61us/step - loss: 55.8242\n",
      "Epoch 110/200\n",
      "4521/4521 [==============================] - 0s 68us/step - loss: 60.4115\n",
      "Epoch 111/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 55.8613\n",
      "Epoch 112/200\n",
      "4521/4521 [==============================] - 0s 73us/step - loss: 57.3408\n",
      "Epoch 113/200\n",
      "4521/4521 [==============================] - 0s 68us/step - loss: 57.4361\n",
      "Epoch 114/200\n",
      "4521/4521 [==============================] - 0s 68us/step - loss: 58.2816\n",
      "Epoch 115/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 58.3487\n",
      "Epoch 116/200\n",
      "4521/4521 [==============================] - 0s 55us/step - loss: 55.4283\n",
      "Epoch 117/200\n",
      "4521/4521 [==============================] - 0s 48us/step - loss: 56.7505\n",
      "Epoch 118/200\n",
      "4521/4521 [==============================] - 0s 55us/step - loss: 56.7708\n",
      "Epoch 119/200\n",
      "4521/4521 [==============================] - 0s 52us/step - loss: 57.8205\n",
      "Epoch 120/200\n",
      "4521/4521 [==============================] - 0s 51us/step - loss: 59.4889\n",
      "Epoch 121/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 56.6996\n",
      "Epoch 122/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 55.3897\n",
      "Epoch 123/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 57.6028\n",
      "Epoch 124/200\n",
      "4521/4521 [==============================] - 0s 66us/step - loss: 56.6016\n",
      "Epoch 125/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 56.5571\n",
      "Epoch 126/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 56.2056\n",
      "Epoch 127/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 55.0608\n",
      "Epoch 128/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 53.7660\n",
      "Epoch 129/200\n",
      "4521/4521 [==============================] - 0s 69us/step - loss: 55.5015\n",
      "Epoch 130/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 55.5118\n",
      "Epoch 131/200\n",
      "4521/4521 [==============================] - 0s 64us/step - loss: 56.1232\n",
      "Epoch 132/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 53.7722\n",
      "Epoch 133/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 57.6349\n",
      "Epoch 134/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 55.8461\n",
      "Epoch 135/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 55.0516\n",
      "Epoch 136/200\n",
      "4521/4521 [==============================] - 0s 67us/step - loss: 55.3567\n",
      "Epoch 137/200\n",
      "4521/4521 [==============================] - 0s 66us/step - loss: 54.8937\n",
      "Epoch 138/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 56.8193\n",
      "Epoch 139/200\n",
      "4521/4521 [==============================] - 0s 51us/step - loss: 55.7024\n",
      "Epoch 140/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 55.5468\n",
      "Epoch 141/200\n",
      "4521/4521 [==============================] - 0s 69us/step - loss: 55.9114\n",
      "Epoch 142/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 52.4203\n",
      "Epoch 143/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 54.3565\n",
      "Epoch 144/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 54.7229\n",
      "Epoch 145/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 52.1442\n",
      "Epoch 146/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 52.6654\n",
      "Epoch 147/200\n",
      "4521/4521 [==============================] - 0s 56us/step - loss: 51.8903\n",
      "Epoch 148/200\n",
      "4521/4521 [==============================] - 0s 66us/step - loss: 51.8628\n",
      "Epoch 149/200\n",
      "4521/4521 [==============================] - 0s 53us/step - loss: 54.0264\n",
      "Epoch 150/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 53.0848\n",
      "Epoch 151/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 52.0562\n",
      "Epoch 152/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 54.1161\n",
      "Epoch 153/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 53.9887\n",
      "Epoch 154/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 52.9090\n",
      "Epoch 155/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 50.7907\n",
      "Epoch 156/200\n",
      "4521/4521 [==============================] - 0s 64us/step - loss: 55.2331\n",
      "Epoch 157/200\n",
      "4521/4521 [==============================] - 0s 74us/step - loss: 52.6032\n",
      "Epoch 158/200\n",
      "4521/4521 [==============================] - 0s 67us/step - loss: 52.9650\n",
      "Epoch 159/200\n",
      "4521/4521 [==============================] - 0s 65us/step - loss: 52.1997\n",
      "Epoch 160/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 52.2531\n",
      "Epoch 161/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 52.4851\n",
      "Epoch 162/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 52.3767\n",
      "Epoch 163/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 50.2471\n",
      "Epoch 164/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 53.2849\n",
      "Epoch 165/200\n",
      "4521/4521 [==============================] - 0s 61us/step - loss: 52.4002\n",
      "Epoch 166/200\n",
      "4521/4521 [==============================] - 0s 71us/step - loss: 53.9903\n",
      "Epoch 167/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 53.5441\n",
      "Epoch 168/200\n",
      "4521/4521 [==============================] - 0s 53us/step - loss: 50.3203\n",
      "Epoch 169/200\n",
      "4521/4521 [==============================] - 0s 66us/step - loss: 51.6758\n",
      "Epoch 170/200\n",
      "4521/4521 [==============================] - 0s 61us/step - loss: 52.7255\n",
      "Epoch 171/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 50.6115\n",
      "Epoch 172/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 49.9948\n",
      "Epoch 173/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 51.2434\n",
      "Epoch 174/200\n",
      "4521/4521 [==============================] - 0s 71us/step - loss: 50.3278\n",
      "Epoch 175/200\n",
      "4521/4521 [==============================] - 0s 66us/step - loss: 54.6952\n",
      "Epoch 176/200\n",
      "4521/4521 [==============================] - 0s 65us/step - loss: 51.3407\n",
      "Epoch 177/200\n",
      "4521/4521 [==============================] - 0s 69us/step - loss: 51.8136\n",
      "Epoch 178/200\n",
      "4521/4521 [==============================] - 0s 53us/step - loss: 51.9885\n",
      "Epoch 179/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 52.3247\n",
      "Epoch 180/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 50.4556\n",
      "Epoch 181/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 49.7033\n",
      "Epoch 182/200\n",
      "4521/4521 [==============================] - 0s 61us/step - loss: 52.1158\n",
      "Epoch 183/200\n",
      "4521/4521 [==============================] - 0s 59us/step - loss: 50.5100\n",
      "Epoch 184/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 52.3719\n",
      "Epoch 185/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 49.4192\n",
      "Epoch 186/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 48.8995\n",
      "Epoch 187/200\n",
      "4521/4521 [==============================] - 0s 55us/step - loss: 50.7743\n",
      "Epoch 188/200\n",
      "4521/4521 [==============================] - 0s 55us/step - loss: 48.3458\n",
      "Epoch 189/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 49.6837\n",
      "Epoch 190/200\n",
      "4521/4521 [==============================] - 0s 67us/step - loss: 48.8767\n",
      "Epoch 191/200\n",
      "4521/4521 [==============================] - 0s 57us/step - loss: 52.2221\n",
      "Epoch 192/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 51.8686\n",
      "Epoch 193/200\n",
      "4521/4521 [==============================] - 0s 58us/step - loss: 48.8771\n",
      "Epoch 194/200\n",
      "4521/4521 [==============================] - 0s 72us/step - loss: 49.7844\n",
      "Epoch 195/200\n",
      "4521/4521 [==============================] - 0s 66us/step - loss: 49.0800\n",
      "Epoch 196/200\n",
      "4521/4521 [==============================] - 0s 62us/step - loss: 49.6685\n",
      "Epoch 197/200\n",
      "4521/4521 [==============================] - 0s 54us/step - loss: 48.4225\n",
      "Epoch 198/200\n",
      "4521/4521 [==============================] - 0s 60us/step - loss: 47.8251\n",
      "Epoch 199/200\n",
      "4521/4521 [==============================] - 0s 71us/step - loss: 49.8032\n",
      "Epoch 200/200\n",
      "4521/4521 [==============================] - 0s 51us/step - loss: 47.6991\n",
      "4521/4521 [==============================] - 1s 196us/step\n",
      "training_set rmse: 5.015159\n",
      "validation_set rmse: 6.056440\n"
     ]
    }
   ],
   "source": [
    "# 用Keras搭建Regression的神经网络\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x_train = x_train_set\n",
    "y_train = y_train_set\n",
    "# DNN 100 5\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim = len(x_train[0]), units = 50, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1))\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(x_train, y_train, batch_size = 100, epochs = 200)\n",
    "result_train = model.evaluate(x_train, y_train)\n",
    "result_train\n",
    "\n",
    "rmse_train = np.sqrt(np.sum(np.power(model.predict(x_train)-y_train, 2)) / (len(y_train)))\n",
    "print('training_set rmse: %6f'%rmse_train)\n",
    "predict_y_validation_set = model.predict(x_validation_set)\n",
    "rmse = np.sqrt(np.sum(np.power(predict_y_validation_set - y_validation_set, 2)) / (len(y_validation_set)))\n",
    "print('validation_set rmse: %6f'%rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存model\n",
    "# model.save('regression_keras1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set rmse: 5.015159\n",
      "validation_set rmse: 6.056440\n"
     ]
    }
   ],
   "source": [
    "# 载入保存的model，测试得到的结果与之前训练的是否相同\n",
    "from keras.models import load_model\n",
    "model1=load_model('regression_keras1.h5')\n",
    "\n",
    "rmse_train = np.sqrt(np.sum(np.power(model1.predict(x_train)-y_train, 2)) / (len(y_train)))\n",
    "print('training_set rmse: %6f'%rmse_train)\n",
    "predict_y_validation_set = model1.predict(x_validation_set)\n",
    "rmse = np.sqrt(np.sum(np.power(predict_y_validation_set - y_validation_set, 2)) / (len(y_validation_set)))\n",
    "print('validation_set rmse: %6f'%rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (None, 50)                8150      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 10,751\n",
      "Trainable params: 10,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5652/5652 [==============================] - 3s 508us/step - loss: 620.0176\n",
      "Epoch 2/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 354.7516\n",
      "Epoch 3/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 215.4141\n",
      "Epoch 4/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 180.5705\n",
      "Epoch 5/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 160.0339\n",
      "Epoch 6/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 146.4372\n",
      "Epoch 7/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 132.5568\n",
      "Epoch 8/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 121.5340\n",
      "Epoch 9/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 118.1646\n",
      "Epoch 10/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 111.5174\n",
      "Epoch 11/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 108.1376\n",
      "Epoch 12/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 106.8472\n",
      "Epoch 13/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 106.3746\n",
      "Epoch 14/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 100.0635\n",
      "Epoch 15/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 98.5348\n",
      "Epoch 16/200\n",
      "5652/5652 [==============================] - 0s 57us/step - loss: 99.2005\n",
      "Epoch 17/200\n",
      "5652/5652 [==============================] - 0s 62us/step - loss: 93.9708\n",
      "Epoch 18/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 89.9978\n",
      "Epoch 19/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 89.6290\n",
      "Epoch 20/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 86.7440\n",
      "Epoch 21/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 84.0633\n",
      "Epoch 22/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 86.4312\n",
      "Epoch 23/200\n",
      "5652/5652 [==============================] - 0s 49us/step - loss: 83.1064\n",
      "Epoch 24/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 79.9002\n",
      "Epoch 25/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 80.4495\n",
      "Epoch 26/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 83.2086\n",
      "Epoch 27/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 81.8101\n",
      "Epoch 28/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 80.7286\n",
      "Epoch 29/200\n",
      "5652/5652 [==============================] - 0s 50us/step - loss: 81.3241\n",
      "Epoch 30/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 75.8432\n",
      "Epoch 31/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 74.3444\n",
      "Epoch 32/200\n",
      "5652/5652 [==============================] - 0s 64us/step - loss: 74.1672\n",
      "Epoch 33/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 73.8211\n",
      "Epoch 34/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 73.8175\n",
      "Epoch 35/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 74.9063\n",
      "Epoch 36/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 72.3883\n",
      "Epoch 37/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 75.5009\n",
      "Epoch 38/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 71.5939\n",
      "Epoch 39/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 73.2391\n",
      "Epoch 40/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 70.0376\n",
      "Epoch 41/200\n",
      "5652/5652 [==============================] - 0s 57us/step - loss: 70.9187\n",
      "Epoch 42/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 68.0008\n",
      "Epoch 43/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 71.5009\n",
      "Epoch 44/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 69.5529\n",
      "Epoch 45/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 71.2179\n",
      "Epoch 46/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 70.3449\n",
      "Epoch 47/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 67.4835\n",
      "Epoch 48/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 69.5192\n",
      "Epoch 49/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 67.9042\n",
      "Epoch 50/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 66.1270\n",
      "Epoch 51/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 64.2362\n",
      "Epoch 52/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 64.8900\n",
      "Epoch 53/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 69.0102\n",
      "Epoch 54/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 68.3275\n",
      "Epoch 55/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 67.4526\n",
      "Epoch 56/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 65.7370\n",
      "Epoch 57/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 65.2971\n",
      "Epoch 58/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 64.3129\n",
      "Epoch 59/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 62.3479\n",
      "Epoch 60/200\n",
      "5652/5652 [==============================] - 0s 60us/step - loss: 60.1279\n",
      "Epoch 61/200\n",
      "5652/5652 [==============================] - 0s 50us/step - loss: 64.6325\n",
      "Epoch 62/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 67.1205\n",
      "Epoch 63/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 65.8183\n",
      "Epoch 64/200\n",
      "5652/5652 [==============================] - 0s 65us/step - loss: 62.7828\n",
      "Epoch 65/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 63.7990\n",
      "Epoch 66/200\n",
      "5652/5652 [==============================] - 0s 50us/step - loss: 62.9043\n",
      "Epoch 67/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 61.5310\n",
      "Epoch 68/200\n",
      "5652/5652 [==============================] - 0s 62us/step - loss: 62.4701\n",
      "Epoch 69/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 64.3851\n",
      "Epoch 70/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 62.1904\n",
      "Epoch 71/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 61.7247\n",
      "Epoch 72/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 62.1649\n",
      "Epoch 73/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 61.1101\n",
      "Epoch 74/200\n",
      "5652/5652 [==============================] - 0s 57us/step - loss: 60.9593\n",
      "Epoch 75/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 61.2405\n",
      "Epoch 76/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 58.6384\n",
      "Epoch 77/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 60.1583\n",
      "Epoch 78/200\n",
      "5652/5652 [==============================] - 0s 63us/step - loss: 59.9987\n",
      "Epoch 79/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 58.6323\n",
      "Epoch 80/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 59.0776\n",
      "Epoch 81/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 61.3489\n",
      "Epoch 82/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 62.3788\n",
      "Epoch 83/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 62.8210\n",
      "Epoch 84/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 57.8584\n",
      "Epoch 85/200\n",
      "5652/5652 [==============================] - 0s 64us/step - loss: 57.0169\n",
      "Epoch 86/200\n",
      "5652/5652 [==============================] - 0s 49us/step - loss: 62.2777\n",
      "Epoch 87/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 57.5764\n",
      "Epoch 88/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 62.2345\n",
      "Epoch 89/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 59.2211\n",
      "Epoch 90/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 57.7403\n",
      "Epoch 91/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 60.6117\n",
      "Epoch 92/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 57.6462\n",
      "Epoch 93/200\n",
      "5652/5652 [==============================] - 0s 57us/step - loss: 58.1329\n",
      "Epoch 94/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 58.3911\n",
      "Epoch 95/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 56.8640\n",
      "Epoch 96/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 58.6073\n",
      "Epoch 97/200\n",
      "5652/5652 [==============================] - 0s 63us/step - loss: 56.5258\n",
      "Epoch 98/200\n",
      "5652/5652 [==============================] - 0s 71us/step - loss: 57.3259\n",
      "Epoch 99/200\n",
      "5652/5652 [==============================] - 0s 49us/step - loss: 59.5694\n",
      "Epoch 100/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 55.6220\n",
      "Epoch 101/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 55.4135\n",
      "Epoch 102/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 58.9093\n",
      "Epoch 103/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 57.5328\n",
      "Epoch 104/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 56.0847\n",
      "Epoch 105/200\n",
      "5652/5652 [==============================] - 0s 60us/step - loss: 55.8281\n",
      "Epoch 106/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 58.1549\n",
      "Epoch 107/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 57.6431\n",
      "Epoch 108/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 54.4378\n",
      "Epoch 109/200\n",
      "5652/5652 [==============================] - 0s 63us/step - loss: 54.5039\n",
      "Epoch 110/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 53.4584\n",
      "Epoch 111/200\n",
      "5652/5652 [==============================] - 0s 50us/step - loss: 54.6517\n",
      "Epoch 112/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 57.6959\n",
      "Epoch 113/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 56.6972\n",
      "Epoch 114/200\n",
      "5652/5652 [==============================] - 0s 63us/step - loss: 55.1047\n",
      "Epoch 115/200\n",
      "5652/5652 [==============================] - 0s 62us/step - loss: 51.2995\n",
      "Epoch 116/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 56.0766\n",
      "Epoch 117/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 54.0180\n",
      "Epoch 118/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 53.6080\n",
      "Epoch 119/200\n",
      "5652/5652 [==============================] - 0s 73us/step - loss: 55.0667\n",
      "Epoch 120/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 56.1713\n",
      "Epoch 121/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 55.0484\n",
      "Epoch 122/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 56.0131\n",
      "Epoch 123/200\n",
      "5652/5652 [==============================] - 0s 51us/step - loss: 54.3592\n",
      "Epoch 124/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 56.9451\n",
      "Epoch 125/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 54.7387\n",
      "Epoch 126/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 54.1823\n",
      "Epoch 127/200\n",
      "5652/5652 [==============================] - 0s 64us/step - loss: 56.6162\n",
      "Epoch 128/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 53.9845\n",
      "Epoch 129/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 54.2608\n",
      "Epoch 130/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 52.9851\n",
      "Epoch 131/200\n",
      "5652/5652 [==============================] - 0s 65us/step - loss: 56.2944\n",
      "Epoch 132/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 52.0438\n",
      "Epoch 133/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 54.1146\n",
      "Epoch 134/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 53.1203\n",
      "Epoch 135/200\n",
      "5652/5652 [==============================] - 0s 64us/step - loss: 53.3518\n",
      "Epoch 136/200\n",
      "5652/5652 [==============================] - 0s 62us/step - loss: 53.8666\n",
      "Epoch 137/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 54.7838\n",
      "Epoch 138/200\n",
      "5652/5652 [==============================] - 0s 50us/step - loss: 53.8221\n",
      "Epoch 139/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 54.2390\n",
      "Epoch 140/200\n",
      "5652/5652 [==============================] - 0s 50us/step - loss: 52.7999\n",
      "Epoch 141/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 52.2827\n",
      "Epoch 142/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 55.4558\n",
      "Epoch 143/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 53.1122\n",
      "Epoch 144/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 52.8909\n",
      "Epoch 145/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 52.5146\n",
      "Epoch 146/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 51.5423\n",
      "Epoch 147/200\n",
      "5652/5652 [==============================] - 0s 74us/step - loss: 52.7718\n",
      "Epoch 148/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 52.7515\n",
      "Epoch 149/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 52.2458\n",
      "Epoch 150/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 50.6702\n",
      "Epoch 151/200\n",
      "5652/5652 [==============================] - 0s 60us/step - loss: 50.7608\n",
      "Epoch 152/200\n",
      "5652/5652 [==============================] - 0s 68us/step - loss: 50.6653\n",
      "Epoch 153/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 51.4030\n",
      "Epoch 154/200\n",
      "5652/5652 [==============================] - 0s 57us/step - loss: 49.9023\n",
      "Epoch 155/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 49.3699\n",
      "Epoch 156/200\n",
      "5652/5652 [==============================] - 0s 52us/step - loss: 53.0827\n",
      "Epoch 157/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 50.9303\n",
      "Epoch 158/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 50.3010\n",
      "Epoch 159/200\n",
      "5652/5652 [==============================] - 0s 60us/step - loss: 50.6095\n",
      "Epoch 160/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 50.3590\n",
      "Epoch 161/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 50.5580\n",
      "Epoch 162/200\n",
      "5652/5652 [==============================] - 0s 60us/step - loss: 51.5521\n",
      "Epoch 163/200\n",
      "5652/5652 [==============================] - 0s 64us/step - loss: 50.9207\n",
      "Epoch 164/200\n",
      "5652/5652 [==============================] - 0s 63us/step - loss: 50.2482\n",
      "Epoch 165/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 52.1921\n",
      "Epoch 166/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 50.2241\n",
      "Epoch 167/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 48.5201\n",
      "Epoch 168/200\n",
      "5652/5652 [==============================] - 0s 63us/step - loss: 51.5870\n",
      "Epoch 169/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 52.5508\n",
      "Epoch 170/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 51.4295\n",
      "Epoch 171/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 50.9763\n",
      "Epoch 172/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 49.1450\n",
      "Epoch 173/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 50.0257\n",
      "Epoch 174/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 48.0901\n",
      "Epoch 175/200\n",
      "5652/5652 [==============================] - 0s 57us/step - loss: 48.9106\n",
      "Epoch 176/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 48.2950\n",
      "Epoch 177/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 48.8509\n",
      "Epoch 178/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 49.8368\n",
      "Epoch 179/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 50.4380\n",
      "Epoch 180/200\n",
      "5652/5652 [==============================] - 0s 60us/step - loss: 49.3905\n",
      "Epoch 181/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 50.1764\n",
      "Epoch 182/200\n",
      "5652/5652 [==============================] - 0s 54us/step - loss: 51.4219\n",
      "Epoch 183/200\n",
      "5652/5652 [==============================] - 0s 55us/step - loss: 50.4956\n",
      "Epoch 184/200\n",
      "5652/5652 [==============================] - 0s 65us/step - loss: 50.2540\n",
      "Epoch 185/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 50.6547\n",
      "Epoch 186/200\n",
      "5652/5652 [==============================] - 0s 64us/step - loss: 47.9866\n",
      "Epoch 187/200\n",
      "5652/5652 [==============================] - 0s 60us/step - loss: 50.4446\n",
      "Epoch 188/200\n",
      "5652/5652 [==============================] - 0s 53us/step - loss: 50.4409\n",
      "Epoch 189/200\n",
      "5652/5652 [==============================] - 0s 60us/step - loss: 50.5542\n",
      "Epoch 190/200\n",
      "5652/5652 [==============================] - 0s 62us/step - loss: 49.3894\n",
      "Epoch 191/200\n",
      "5652/5652 [==============================] - 0s 59us/step - loss: 49.8241\n",
      "Epoch 192/200\n",
      "5652/5652 [==============================] - 0s 56us/step - loss: 48.3497\n",
      "Epoch 193/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 50.2613\n",
      "Epoch 194/200\n",
      "5652/5652 [==============================] - 0s 48us/step - loss: 50.6762\n",
      "Epoch 195/200\n",
      "5652/5652 [==============================] - 0s 69us/step - loss: 47.7293\n",
      "Epoch 196/200\n",
      "5652/5652 [==============================] - 0s 68us/step - loss: 50.9655\n",
      "Epoch 197/200\n",
      "5652/5652 [==============================] - 0s 60us/step - loss: 49.6884\n",
      "Epoch 198/200\n",
      "5652/5652 [==============================] - 0s 58us/step - loss: 48.9761\n",
      "Epoch 199/200\n",
      "5652/5652 [==============================] - 0s 61us/step - loss: 48.6327\n",
      "Epoch 200/200\n",
      "5652/5652 [==============================] - 0s 63us/step - loss: 48.0981\n",
      "5652/5652 [==============================] - 1s 200us/step\n",
      "training_all rmse: 5.173096\n"
     ]
    }
   ],
   "source": [
    "# 用全部的data对model进行训练\n",
    "x_train = x\n",
    "y_train = y\n",
    "# DNN \n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim = len(x_train[0]), units = 50, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1))\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(x_train, y_train, batch_size = 100, epochs = 200)\n",
    "result_train = model.evaluate(x_train, y_train)\n",
    "result_train\n",
    "\n",
    "rmse_train = np.sqrt(np.sum(np.power(model.predict(x_train)-y_train, 2)) / (len(y_train)))\n",
    "print('training_all rmse: %6f'%rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存用全部data训练好的model\n",
    "# model.save('regression_keras_all_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_all rmse: 5.173096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.1135626],\n",
       "       [14.583319 ],\n",
       "       [24.68214  ],\n",
       "       [ 8.194885 ],\n",
       "       [26.445301 ],\n",
       "       [20.36614  ],\n",
       "       [20.671059 ],\n",
       "       [29.487627 ],\n",
       "       [16.720682 ],\n",
       "       [59.626274 ],\n",
       "       [12.809026 ],\n",
       "       [ 9.102818 ],\n",
       "       [61.193687 ],\n",
       "       [56.939335 ],\n",
       "       [20.801842 ],\n",
       "       [ 8.767662 ],\n",
       "       [31.944778 ],\n",
       "       [59.91794  ],\n",
       "       [ 3.1889222],\n",
       "       [13.837721 ],\n",
       "       [47.53346  ],\n",
       "       [69.15253  ],\n",
       "       [ 8.144107 ],\n",
       "       [16.689455 ],\n",
       "       [11.901781 ],\n",
       "       [37.991222 ],\n",
       "       [13.524094 ],\n",
       "       [60.994003 ],\n",
       "       [ 7.1832023],\n",
       "       [56.49576  ],\n",
       "       [23.651724 ],\n",
       "       [ 8.749539 ],\n",
       "       [ 5.6168566],\n",
       "       [18.882483 ],\n",
       "       [27.314148 ],\n",
       "       [36.543365 ],\n",
       "       [48.920345 ],\n",
       "       [32.835575 ],\n",
       "       [43.499268 ],\n",
       "       [32.53605  ],\n",
       "       [ 8.571181 ],\n",
       "       [44.239532 ],\n",
       "       [30.386227 ],\n",
       "       [53.026646 ],\n",
       "       [14.831295 ],\n",
       "       [34.76526  ],\n",
       "       [23.139282 ],\n",
       "       [ 8.802496 ],\n",
       "       [24.927853 ],\n",
       "       [29.613737 ],\n",
       "       [20.19213  ],\n",
       "       [ 5.8460245],\n",
       "       [19.722464 ],\n",
       "       [59.169582 ],\n",
       "       [10.987345 ],\n",
       "       [33.78955  ],\n",
       "       [33.19397  ],\n",
       "       [21.232525 ],\n",
       "       [58.124004 ],\n",
       "       [20.197441 ],\n",
       "       [14.228247 ],\n",
       "       [37.51931  ],\n",
       "       [10.250509 ],\n",
       "       [51.343975 ],\n",
       "       [10.812716 ],\n",
       "       [10.515249 ],\n",
       "       [14.180225 ],\n",
       "       [ 2.8939233],\n",
       "       [46.940796 ],\n",
       "       [27.739286 ],\n",
       "       [15.898159 ],\n",
       "       [42.33639  ],\n",
       "       [58.63152  ],\n",
       "       [ 4.513604 ],\n",
       "       [16.164001 ],\n",
       "       [ 7.0837584],\n",
       "       [40.051556 ],\n",
       "       [13.636148 ],\n",
       "       [22.638426 ],\n",
       "       [19.456474 ],\n",
       "       [23.049114 ],\n",
       "       [33.137856 ],\n",
       "       [20.997227 ],\n",
       "       [73.55371  ],\n",
       "       [40.19113  ],\n",
       "       [28.069027 ],\n",
       "       [21.718378 ],\n",
       "       [34.449875 ],\n",
       "       [22.583265 ],\n",
       "       [20.930647 ],\n",
       "       [29.4912   ],\n",
       "       [42.609184 ],\n",
       "       [ 6.229617 ],\n",
       "       [38.929825 ],\n",
       "       [49.41503  ],\n",
       "       [12.626053 ],\n",
       "       [33.325466 ],\n",
       "       [11.170627 ],\n",
       "       [20.545574 ],\n",
       "       [ 4.5413256],\n",
       "       [17.265564 ],\n",
       "       [22.974052 ],\n",
       "       [12.726643 ],\n",
       "       [13.345005 ],\n",
       "       [21.286905 ],\n",
       "       [37.58647  ],\n",
       "       [35.20515  ],\n",
       "       [ 7.443122 ],\n",
       "       [ 5.0157332],\n",
       "       [67.62556  ],\n",
       "       [51.68863  ],\n",
       "       [13.762075 ],\n",
       "       [25.225431 ],\n",
       "       [17.870207 ],\n",
       "       [10.675396 ],\n",
       "       [24.106346 ],\n",
       "       [25.038164 ],\n",
       "       [ 7.612582 ],\n",
       "       [19.011684 ],\n",
       "       [17.957788 ],\n",
       "       [66.74718  ],\n",
       "       [25.693192 ],\n",
       "       [41.93262  ],\n",
       "       [24.71543  ],\n",
       "       [ 8.178045 ],\n",
       "       [39.61999  ],\n",
       "       [ 9.044561 ],\n",
       "       [21.982494 ],\n",
       "       [29.062634 ],\n",
       "       [66.15438  ],\n",
       "       [15.910658 ],\n",
       "       [23.45237  ],\n",
       "       [59.56351  ],\n",
       "       [13.776272 ],\n",
       "       [14.294352 ],\n",
       "       [ 4.2831635],\n",
       "       [12.131676 ],\n",
       "       [58.360546 ],\n",
       "       [13.352718 ],\n",
       "       [ 3.4776378],\n",
       "       [28.24257  ],\n",
       "       [24.150856 ],\n",
       "       [51.31279  ],\n",
       "       [32.3843   ],\n",
       "       [14.338402 ],\n",
       "       [27.192352 ],\n",
       "       [ 6.4723506],\n",
       "       [54.212135 ],\n",
       "       [22.71177  ],\n",
       "       [37.428867 ],\n",
       "       [ 8.143929 ],\n",
       "       [ 7.014602 ],\n",
       "       [23.00697  ],\n",
       "       [ 4.71459  ],\n",
       "       [16.334564 ],\n",
       "       [40.366314 ],\n",
       "       [ 5.9112883],\n",
       "       [35.174213 ],\n",
       "       [14.062437 ],\n",
       "       [16.894737 ],\n",
       "       [40.122437 ],\n",
       "       [15.5467205],\n",
       "       [12.273018 ],\n",
       "       [ 6.645445 ],\n",
       "       [55.005608 ],\n",
       "       [32.754597 ],\n",
       "       [ 3.2148004],\n",
       "       [12.578863 ],\n",
       "       [62.65606  ],\n",
       "       [11.88245  ],\n",
       "       [63.638206 ],\n",
       "       [46.525562 ],\n",
       "       [24.17911  ],\n",
       "       [17.279675 ],\n",
       "       [60.03405  ],\n",
       "       [25.835424 ],\n",
       "       [21.895348 ],\n",
       "       [40.55596  ],\n",
       "       [11.29399  ],\n",
       "       [31.667221 ],\n",
       "       [15.157877 ],\n",
       "       [ 9.217202 ],\n",
       "       [62.033447 ],\n",
       "       [47.216988 ],\n",
       "       [12.06107  ],\n",
       "       [37.063316 ],\n",
       "       [24.289768 ],\n",
       "       [63.69639  ],\n",
       "       [10.686665 ],\n",
       "       [53.525063 ],\n",
       "       [40.347008 ],\n",
       "       [16.475945 ],\n",
       "       [29.50711  ],\n",
       "       [ 3.7276216],\n",
       "       [19.983809 ],\n",
       "       [ 3.7001445],\n",
       "       [33.86533  ],\n",
       "       [11.407    ],\n",
       "       [21.376347 ],\n",
       "       [54.86161  ],\n",
       "       [23.626637 ],\n",
       "       [14.658813 ],\n",
       "       [61.661865 ],\n",
       "       [ 9.003374 ],\n",
       "       [ 8.269989 ],\n",
       "       [ 5.769976 ],\n",
       "       [ 7.413356 ],\n",
       "       [ 3.8030362],\n",
       "       [75.926956 ],\n",
       "       [18.33975  ],\n",
       "       [10.842524 ],\n",
       "       [14.667034 ],\n",
       "       [33.07742  ],\n",
       "       [35.92032  ],\n",
       "       [11.412228 ],\n",
       "       [35.30315  ],\n",
       "       [64.58628  ],\n",
       "       [ 3.8702238],\n",
       "       [ 9.299576 ],\n",
       "       [33.013916 ],\n",
       "       [11.445051 ],\n",
       "       [11.942915 ],\n",
       "       [71.13687  ],\n",
       "       [13.410028 ],\n",
       "       [16.404728 ],\n",
       "       [60.535946 ],\n",
       "       [14.320946 ],\n",
       "       [17.473898 ],\n",
       "       [ 6.4465303],\n",
       "       [ 8.139275 ],\n",
       "       [41.391243 ],\n",
       "       [11.888329 ],\n",
       "       [57.36317  ],\n",
       "       [41.100357 ],\n",
       "       [20.903982 ],\n",
       "       [44.7493   ],\n",
       "       [62.086605 ],\n",
       "       [37.6818   ],\n",
       "       [10.014658 ],\n",
       "       [14.34733  ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载model并预测\n",
    "model_all_data = load_model('regression_keras_all_data.h5')\n",
    "rmse_train = np.sqrt(np.sum(np.power(model_all_data.predict(x_train)-y_train, 2)) / (len(y_train)))\n",
    "print('training_all rmse: %6f'%rmse_train)\n",
    "\n",
    "predict_y_keras = model_all_data.predict(test_x)\n",
    "predict_y_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-HVDMDOjD1st",
    "outputId": "982ea6e6-1cba-450d-d2cd-0b8443660367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'value']\n",
      "['id_0', 7.1135626]\n",
      "['id_1', 14.583319]\n",
      "['id_2', 24.68214]\n",
      "['id_3', 8.194885]\n",
      "['id_4', 26.445301]\n",
      "['id_5', 20.36614]\n",
      "['id_6', 20.671059]\n",
      "['id_7', 29.487627]\n",
      "['id_8', 16.720682]\n",
      "['id_9', 59.626274]\n",
      "['id_10', 12.809026]\n",
      "['id_11', 9.102818]\n",
      "['id_12', 61.193687]\n",
      "['id_13', 56.939335]\n",
      "['id_14', 20.801842]\n",
      "['id_15', 8.767662]\n",
      "['id_16', 31.944778]\n",
      "['id_17', 59.91794]\n",
      "['id_18', 3.1889222]\n",
      "['id_19', 13.837721]\n",
      "['id_20', 47.53346]\n",
      "['id_21', 69.15253]\n",
      "['id_22', 8.144107]\n",
      "['id_23', 16.689455]\n",
      "['id_24', 11.901781]\n",
      "['id_25', 37.991222]\n",
      "['id_26', 13.524094]\n",
      "['id_27', 60.994003]\n",
      "['id_28', 7.1832023]\n",
      "['id_29', 56.49576]\n",
      "['id_30', 23.651724]\n",
      "['id_31', 8.749539]\n",
      "['id_32', 5.6168566]\n",
      "['id_33', 18.882483]\n",
      "['id_34', 27.314148]\n",
      "['id_35', 36.543365]\n",
      "['id_36', 48.920345]\n",
      "['id_37', 32.835575]\n",
      "['id_38', 43.499268]\n",
      "['id_39', 32.53605]\n",
      "['id_40', 8.571181]\n",
      "['id_41', 44.239532]\n",
      "['id_42', 30.386227]\n",
      "['id_43', 53.026646]\n",
      "['id_44', 14.831295]\n",
      "['id_45', 34.76526]\n",
      "['id_46', 23.139282]\n",
      "['id_47', 8.802496]\n",
      "['id_48', 24.927853]\n",
      "['id_49', 29.613737]\n",
      "['id_50', 20.19213]\n",
      "['id_51', 5.8460245]\n",
      "['id_52', 19.722464]\n",
      "['id_53', 59.169582]\n",
      "['id_54', 10.987345]\n",
      "['id_55', 33.78955]\n",
      "['id_56', 33.19397]\n",
      "['id_57', 21.232525]\n",
      "['id_58', 58.124004]\n",
      "['id_59', 20.197441]\n",
      "['id_60', 14.228247]\n",
      "['id_61', 37.51931]\n",
      "['id_62', 10.250509]\n",
      "['id_63', 51.343975]\n",
      "['id_64', 10.812716]\n",
      "['id_65', 10.515249]\n",
      "['id_66', 14.180225]\n",
      "['id_67', 2.8939233]\n",
      "['id_68', 46.940796]\n",
      "['id_69', 27.739286]\n",
      "['id_70', 15.898159]\n",
      "['id_71', 42.33639]\n",
      "['id_72', 58.63152]\n",
      "['id_73', 4.513604]\n",
      "['id_74', 16.164001]\n",
      "['id_75', 7.0837584]\n",
      "['id_76', 40.051556]\n",
      "['id_77', 13.636148]\n",
      "['id_78', 22.638426]\n",
      "['id_79', 19.456474]\n",
      "['id_80', 23.049114]\n",
      "['id_81', 33.137856]\n",
      "['id_82', 20.997227]\n",
      "['id_83', 73.55371]\n",
      "['id_84', 40.19113]\n",
      "['id_85', 28.069027]\n",
      "['id_86', 21.718378]\n",
      "['id_87', 34.449875]\n",
      "['id_88', 22.583265]\n",
      "['id_89', 20.930647]\n",
      "['id_90', 29.4912]\n",
      "['id_91', 42.609184]\n",
      "['id_92', 6.229617]\n",
      "['id_93', 38.929825]\n",
      "['id_94', 49.41503]\n",
      "['id_95', 12.626053]\n",
      "['id_96', 33.325466]\n",
      "['id_97', 11.170627]\n",
      "['id_98', 20.545574]\n",
      "['id_99', 4.5413256]\n",
      "['id_100', 17.265564]\n",
      "['id_101', 22.974052]\n",
      "['id_102', 12.726643]\n",
      "['id_103', 13.345005]\n",
      "['id_104', 21.286905]\n",
      "['id_105', 37.58647]\n",
      "['id_106', 35.20515]\n",
      "['id_107', 7.443122]\n",
      "['id_108', 5.0157332]\n",
      "['id_109', 67.62556]\n",
      "['id_110', 51.68863]\n",
      "['id_111', 13.762075]\n",
      "['id_112', 25.225431]\n",
      "['id_113', 17.870207]\n",
      "['id_114', 10.675396]\n",
      "['id_115', 24.106346]\n",
      "['id_116', 25.038164]\n",
      "['id_117', 7.612582]\n",
      "['id_118', 19.011684]\n",
      "['id_119', 17.957788]\n",
      "['id_120', 66.74718]\n",
      "['id_121', 25.693192]\n",
      "['id_122', 41.93262]\n",
      "['id_123', 24.71543]\n",
      "['id_124', 8.178045]\n",
      "['id_125', 39.61999]\n",
      "['id_126', 9.044561]\n",
      "['id_127', 21.982494]\n",
      "['id_128', 29.062634]\n",
      "['id_129', 66.15438]\n",
      "['id_130', 15.910658]\n",
      "['id_131', 23.45237]\n",
      "['id_132', 59.56351]\n",
      "['id_133', 13.776272]\n",
      "['id_134', 14.294352]\n",
      "['id_135', 4.2831635]\n",
      "['id_136', 12.131676]\n",
      "['id_137', 58.360546]\n",
      "['id_138', 13.352718]\n",
      "['id_139', 3.4776378]\n",
      "['id_140', 28.24257]\n",
      "['id_141', 24.150856]\n",
      "['id_142', 51.31279]\n",
      "['id_143', 32.3843]\n",
      "['id_144', 14.338402]\n",
      "['id_145', 27.192352]\n",
      "['id_146', 6.4723506]\n",
      "['id_147', 54.212135]\n",
      "['id_148', 22.71177]\n",
      "['id_149', 37.428867]\n",
      "['id_150', 8.143929]\n",
      "['id_151', 7.014602]\n",
      "['id_152', 23.00697]\n",
      "['id_153', 4.71459]\n",
      "['id_154', 16.334564]\n",
      "['id_155', 40.366314]\n",
      "['id_156', 5.9112883]\n",
      "['id_157', 35.174213]\n",
      "['id_158', 14.062437]\n",
      "['id_159', 16.894737]\n",
      "['id_160', 40.122437]\n",
      "['id_161', 15.5467205]\n",
      "['id_162', 12.273018]\n",
      "['id_163', 6.645445]\n",
      "['id_164', 55.005608]\n",
      "['id_165', 32.754597]\n",
      "['id_166', 3.2148004]\n",
      "['id_167', 12.578863]\n",
      "['id_168', 62.65606]\n",
      "['id_169', 11.88245]\n",
      "['id_170', 63.638206]\n",
      "['id_171', 46.525562]\n",
      "['id_172', 24.17911]\n",
      "['id_173', 17.279675]\n",
      "['id_174', 60.03405]\n",
      "['id_175', 25.835424]\n",
      "['id_176', 21.895348]\n",
      "['id_177', 40.55596]\n",
      "['id_178', 11.29399]\n",
      "['id_179', 31.667221]\n",
      "['id_180', 15.157877]\n",
      "['id_181', 9.217202]\n",
      "['id_182', 62.033447]\n",
      "['id_183', 47.216988]\n",
      "['id_184', 12.06107]\n",
      "['id_185', 37.063316]\n",
      "['id_186', 24.289768]\n",
      "['id_187', 63.69639]\n",
      "['id_188', 10.686665]\n",
      "['id_189', 53.525063]\n",
      "['id_190', 40.347008]\n",
      "['id_191', 16.475945]\n",
      "['id_192', 29.50711]\n",
      "['id_193', 3.7276216]\n",
      "['id_194', 19.983809]\n",
      "['id_195', 3.7001445]\n",
      "['id_196', 33.86533]\n",
      "['id_197', 11.407]\n",
      "['id_198', 21.376347]\n",
      "['id_199', 54.86161]\n",
      "['id_200', 23.626637]\n",
      "['id_201', 14.658813]\n",
      "['id_202', 61.661865]\n",
      "['id_203', 9.003374]\n",
      "['id_204', 8.269989]\n",
      "['id_205', 5.769976]\n",
      "['id_206', 7.413356]\n",
      "['id_207', 3.8030362]\n",
      "['id_208', 75.926956]\n",
      "['id_209', 18.33975]\n",
      "['id_210', 10.842524]\n",
      "['id_211', 14.667034]\n",
      "['id_212', 33.07742]\n",
      "['id_213', 35.92032]\n",
      "['id_214', 11.412228]\n",
      "['id_215', 35.30315]\n",
      "['id_216', 64.58628]\n",
      "['id_217', 3.8702238]\n",
      "['id_218', 9.299576]\n",
      "['id_219', 33.013916]\n",
      "['id_220', 11.445051]\n",
      "['id_221', 11.942915]\n",
      "['id_222', 71.13687]\n",
      "['id_223', 13.410028]\n",
      "['id_224', 16.404728]\n",
      "['id_225', 60.535946]\n",
      "['id_226', 14.320946]\n",
      "['id_227', 17.473898]\n",
      "['id_228', 6.4465303]\n",
      "['id_229', 8.139275]\n",
      "['id_230', 41.391243]\n",
      "['id_231', 11.888329]\n",
      "['id_232', 57.36317]\n",
      "['id_233', 41.100357]\n",
      "['id_234', 20.903982]\n",
      "['id_235', 44.7493]\n",
      "['id_236', 62.086605]\n",
      "['id_237', 37.6818]\n",
      "['id_238', 10.014658]\n",
      "['id_239', 14.34733]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('predict_keras_dropout_all.csv', mode='w', newline='') as predict_file:\n",
    "    csv_writer = csv.writer(predict_file)\n",
    "    header = ['id', 'value']\n",
    "    print(header)\n",
    "    csv_writer.writerow(header)\n",
    "    for i in range(240):\n",
    "        row = ['id_' + str(i), predict_y_keras[i][0]]\n",
    "        csv_writer.writerow(row)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zCSf7FNECFY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw1_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('ml': conda)",
   "language": "python",
   "name": "python36864bitmlconda4727b916de1c4a8f8265036fed6d2bb8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
