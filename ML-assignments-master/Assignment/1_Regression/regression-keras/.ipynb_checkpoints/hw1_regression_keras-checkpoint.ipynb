{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>日期</th>\n",
       "      <th>測站</th>\n",
       "      <th>測項</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>AMB_TEMP</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>CH4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>CO</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>NMHC</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014/1/1</td>\n",
       "      <td>豐原</td>\n",
       "      <td>NO</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>THC</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>WD_HR</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>44</td>\n",
       "      <td>55</td>\n",
       "      <td>68</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>308</td>\n",
       "      <td>327</td>\n",
       "      <td>21</td>\n",
       "      <td>100</td>\n",
       "      <td>109</td>\n",
       "      <td>108</td>\n",
       "      <td>114</td>\n",
       "      <td>108</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>WIND_DIREC</td>\n",
       "      <td>36</td>\n",
       "      <td>55</td>\n",
       "      <td>72</td>\n",
       "      <td>327</td>\n",
       "      <td>74</td>\n",
       "      <td>52</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>311</td>\n",
       "      <td>52</td>\n",
       "      <td>54</td>\n",
       "      <td>121</td>\n",
       "      <td>97</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>100</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>WIND_SPEED</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>2014/12/20</td>\n",
       "      <td>豐原</td>\n",
       "      <td>WS_HR</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4320 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              日期  測站          測項     0     1     2     3     4     5     6  \\\n",
       "0       2014/1/1  豐原    AMB_TEMP    14    14    14    13    12    12    12   \n",
       "1       2014/1/1  豐原         CH4   1.8   1.8   1.8   1.8   1.8   1.8   1.8   \n",
       "2       2014/1/1  豐原          CO  0.51  0.41  0.39  0.37  0.35   0.3  0.37   \n",
       "3       2014/1/1  豐原        NMHC   0.2  0.15  0.13  0.12  0.11  0.06   0.1   \n",
       "4       2014/1/1  豐原          NO   0.9   0.6   0.5   1.7   1.8   1.5   1.9   \n",
       "...          ...  ..         ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "4315  2014/12/20  豐原         THC   1.8   1.8   1.8   1.8   1.8   1.7   1.7   \n",
       "4316  2014/12/20  豐原       WD_HR    46    13    61    44    55    68    66   \n",
       "4317  2014/12/20  豐原  WIND_DIREC    36    55    72   327    74    52    59   \n",
       "4318  2014/12/20  豐原  WIND_SPEED   1.9   2.4   1.9   2.8   2.3   1.9   2.1   \n",
       "4319  2014/12/20  豐原       WS_HR   0.7   0.8   1.8     1   1.9   1.7   2.1   \n",
       "\n",
       "      ...    14    15    16    17    18    19    20    21    22    23  \n",
       "0     ...    22    22    21    19    17    16    15    15    15    15  \n",
       "1     ...   1.8   1.8   1.8   1.8   1.8   1.8   1.8   1.8   1.8   1.8  \n",
       "2     ...  0.37  0.37  0.47  0.69  0.56  0.45  0.38  0.35  0.36  0.32  \n",
       "3     ...   0.1  0.13  0.14  0.23  0.18  0.12   0.1  0.09   0.1  0.08  \n",
       "4     ...   2.5   2.2   2.5   2.3   2.1   1.9   1.5   1.6   1.8   1.5  \n",
       "...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "4315  ...   1.8   1.8     2   2.1     2   1.9   1.9   1.9     2     2  \n",
       "4316  ...    59   308   327    21   100   109   108   114   108   109  \n",
       "4317  ...    18   311    52    54   121    97   107   118   100   105  \n",
       "4318  ...   2.3   2.6   1.3     1   1.5     1   1.7   1.5     2     2  \n",
       "4319  ...   1.3   1.7   0.7   0.4   1.1   1.4   1.3   1.6   1.8     2  \n",
       "\n",
       "[4320 rows x 27 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data=pd.read_csv('../data/train.csv',encoding='big5')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/pandas/core/frame.py:3530: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._where(-key, value, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['14', '14', '14', ..., '15', '15', '15'],\n",
       "       ['1.8', '1.8', '1.8', ..., '1.8', '1.8', '1.8'],\n",
       "       ['0.51', '0.41', '0.39', ..., '0.35', '0.36', '0.32'],\n",
       "       ...,\n",
       "       ['36', '55', '72', ..., '118', '100', '105'],\n",
       "       ['1.9', '2.4', '1.9', ..., '1.5', '2', '2'],\n",
       "       ['0.7', '0.8', '1.8', ..., '1.6', '1.8', '2']], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们只需要第三列以后的data\n",
    "data = data.iloc[:,3:]\n",
    "# 将NR的数据全部置为0\n",
    "data[data == 'NR'] = 0\n",
    "# 将dataframe幻化成numpy\n",
    "raw_data = data.to_numpy()\n",
    "\n",
    "#查看raw_data\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.  , 14.  , 14.  , ..., 14.  , 13.  , 13.  ],\n",
       "       [ 1.8 ,  1.8 ,  1.8 , ...,  1.8 ,  1.8 ,  1.8 ],\n",
       "       [ 0.51,  0.41,  0.39, ...,  0.34,  0.41,  0.43],\n",
       "       ...,\n",
       "       [35.  , 79.  ,  2.4 , ..., 48.  , 63.  , 53.  ],\n",
       "       [ 1.4 ,  1.8 ,  1.  , ...,  1.1 ,  1.9 ,  1.9 ],\n",
       "       [ 0.5 ,  0.9 ,  0.6 , ...,  1.2 ,  1.2 ,  1.3 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在要开始处理Training data\n",
    "# 由于提供的数据是每个月前20天(每天24小时)的资料(共18种类型)，因此每个月的这20*24=480个小时的时间是连续的，每10个小时可以作为一笔data，480小时共471笔data\n",
    "# 由于不同月份之间的时间是不连续的，因此先把所有的资料按照月份分开\n",
    "month_data = {}\n",
    "for month in range(12):\n",
    "  # 每个月共20*24=480份数据，共18种空气成分，每一种成分每月都有480份数据，因此初始化一个18*480的array\n",
    "  temp_data = np.empty([18,480])\n",
    "  for day in range(20):\n",
    "    # temp_data中加入第day天的数据，由于每天都有24份数据共24列,故列的范围是24*day~24*(day+1)；选择加入temp_data的是第20×month+day这一天的数据，由于每天有18种大气成分的数据共18行，因此行的范围是18*(20*month+day)~18*(20*month+day+1)\n",
    "    temp_data[:, 24 * day : 24 * (day + 1)] = raw_data[18 * (20 * month + day) : 18 * (20 * month + day + 1)]\n",
    "  # 把这个月的data放进month_data[month]里去\n",
    "  month_data[month]=temp_data\n",
    "\n",
    "# 查看第一个月的数据，并借此查看数据的结构：每一行都是一种大气成分在24*20个连续小时内的值\n",
    "month_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.  14.  14.  ...  2.   2.   0.5]\n",
      " [14.  14.  13.  ...  2.   0.5  0.3]\n",
      " [14.  13.  12.  ...  0.5  0.3  0.8]\n",
      " ...\n",
      " [17.  18.  19.  ...  1.1  1.4  1.3]\n",
      " [18.  19.  18.  ...  1.4  1.3  1.6]\n",
      " [19.  18.  17.  ...  1.3  1.6  1.8]]\n",
      "[[30.]\n",
      " [41.]\n",
      " [44.]\n",
      " ...\n",
      " [17.]\n",
      " [24.]\n",
      " [29.]]\n"
     ]
    }
   ],
   "source": [
    "# 这个时候我们就要在12段连续的时间里每10个小时取出一笔Training data\n",
    "# 每个月连续时间有20*24=480h，每10h作为一笔data，共可以分为471笔data；一共12个月，因此共12*471笔data\n",
    "# 每一笔data，前9h为input，最后1h的PM2.5为output，因此input是一个18*9的矩阵，如果把它摊平就是一个18*9的feature vector\n",
    "\n",
    "# x是input，共12*471笔data，每个input是一个18*9的feature vector\n",
    "x = np.empty([12 * 471, 18 * 9], dtype = float)\n",
    "# y是output，共12*471笔data，每个output是第10个小时的第9种大气成分PM2.5，因此只有1维\n",
    "y = np.empty([12 * 471, 1], dtype = float)\n",
    "\n",
    "# 取出input和output\n",
    "for month in range(12):\n",
    "  # 每个月份共471笔data，依次遍历即可\n",
    "  for i in range(471):\n",
    "    # 用一个18行(18种物质),10列(10个小时的数据，包括input和output)的temp_data来存放每一笔Training data\n",
    "    temp_data = np.empty([18, 10])\n",
    "    # 第i笔data的范围是i~i+9，共10列，注意这里i+10是开区间，实际只取到了i+9\n",
    "    temp_data = month_data[month][:, i : i + 10]\n",
    "    # 将temp_data前9列的作为input的数据(18行9列)用reshape平摊到一个18*9的行向量上\n",
    "    x[471 * month + i, :] = temp_data[:, 0 : 9].reshape(1,-1)\n",
    "    # temp_data的第10列作为output，只有第10行的PM2.5值是有用的\n",
    "    y[471 * month + i] = temp_data[9, 9]\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35825331, -1.35883937, -1.359222  , ...,  0.26650729,\n",
       "         0.2656797 , -1.14082131],\n",
       "       [-1.35825331, -1.35883937, -1.51819928, ...,  0.26650729,\n",
       "        -1.13963133, -1.32832904],\n",
       "       [-1.35825331, -1.51789368, -1.67717656, ..., -1.13923451,\n",
       "        -1.32700613, -0.85955971],\n",
       "       ...,\n",
       "       [-0.88092053, -0.72262212, -0.56433559, ..., -0.57693779,\n",
       "        -0.29644471, -0.39079039],\n",
       "       [-0.7218096 , -0.56356781, -0.72331287, ..., -0.29578943,\n",
       "        -0.39013211, -0.1095288 ],\n",
       "       [-0.56269867, -0.72262212, -0.88229015, ..., -0.38950555,\n",
       "        -0.10906991,  0.07797893]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个时候我们已经有了Training data的x和y，但由于feature的每一个dimension大小范围都是不一样的，因此还要对其做feature scale\n",
    "# 求出期望mean和标准差std，利用公式x'=(x-u)/σ来使同一列的数据同时满足同一个分布\n",
    "\n",
    "# 直接调用numpy的mean和std计算期望和标准差，axis=0表示对列进行计算\n",
    "mean_x = np.mean(x, axis = 0)\n",
    "std_x = np.std(x, axis = 0)\n",
    "\n",
    "# 对每一个feature都进行normalize归一化处理\n",
    "for i in range(len(x)):\n",
    "  for j in range(len(x[0])):\n",
    "    if std_x[j] != 0:\n",
    "      x[i][j] = (x[i][j] - mean_x[j])/std_x[j]\n",
    "\n",
    "# 查看normalize后的input x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_set len: 4521\n",
      "\n",
      "[[-1.35825331 -1.35883937 -1.359222   ...  0.26650729  0.2656797\n",
      "  -1.14082131]\n",
      " [-1.35825331 -1.35883937 -1.51819928 ...  0.26650729 -1.13963133\n",
      "  -1.32832904]\n",
      " [-1.35825331 -1.51789368 -1.67717656 ... -1.13923451 -1.32700613\n",
      "  -0.85955971]\n",
      " ...\n",
      " [ 0.86929969  0.70886668  0.38952809 ...  1.39110073  0.2656797\n",
      "  -0.39079039]\n",
      " [ 0.71018876  0.39075806  0.07157353 ...  0.26650729 -0.39013211\n",
      "  -0.39079039]\n",
      " [ 0.3919669   0.07264944  0.07157353 ... -0.38950555 -0.39013211\n",
      "  -0.85955971]]\n",
      "---------------------------------------------------------------------------\n",
      "y_train_set len: 4521\n",
      "\n",
      "[[30.]\n",
      " [41.]\n",
      " [44.]\n",
      " ...\n",
      " [ 7.]\n",
      " [ 5.]\n",
      " [14.]]\n",
      "---------------------------------------------------------------------------\n",
      "x_validation_set len: 1131\n",
      "\n",
      "[[ 0.07374504  0.07264944  0.07157353 ... -0.38950555 -0.85856912\n",
      "  -0.57829812]\n",
      " [ 0.07374504  0.07264944  0.23055081 ... -0.85808615 -0.57750692\n",
      "   0.54674825]\n",
      " [ 0.07374504  0.23170375  0.23055081 ... -0.57693779  0.54674191\n",
      "  -0.1095288 ]\n",
      " ...\n",
      " [-0.88092053 -0.72262212 -0.56433559 ... -0.57693779 -0.29644471\n",
      "  -0.39079039]\n",
      " [-0.7218096  -0.56356781 -0.72331287 ... -0.29578943 -0.39013211\n",
      "  -0.1095288 ]\n",
      " [-0.56269867 -0.72262212 -0.88229015 ... -0.38950555 -0.10906991\n",
      "   0.07797893]]\n",
      "---------------------------------------------------------------------------\n",
      "y_validation_set len: 1131\n",
      "\n",
      "[[13.]\n",
      " [24.]\n",
      " [22.]\n",
      " ...\n",
      " [17.]\n",
      " [24.]\n",
      " [29.]]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 为了有效衡量testing data的bias对结果的影响，这里切出一块validaiton set\n",
    "import math\n",
    "x_train_set = x[: math.floor(len(x) * 0.8), :]\n",
    "y_train_set = y[: math.floor(len(y) * 0.8), :]\n",
    "x_validation_set = x[math.floor(len(x) * 0.8): , :]\n",
    "y_validation_set = y[math.floor(len(y) * 0.8): , :]\n",
    "\n",
    "print('x_train_set len: ' + str(len(x_train_set)) + '\\n')\n",
    "print(x_train_set)\n",
    "print('---------------------------------------------------------------------------')\n",
    "print('y_train_set len: ' + str(len(y_train_set)) + '\\n')\n",
    "print(y_train_set)\n",
    "print('---------------------------------------------------------------------------')\n",
    "print('x_validation_set len: ' + str(len(x_validation_set)) + '\\n')\n",
    "print(x_validation_set)\n",
    "print('---------------------------------------------------------------------------')\n",
    "print('y_validation_set len: ' + str(len(y_validation_set)) + '\\n')\n",
    "print(y_validation_set)\n",
    "print('---------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.24447681, -0.24545919, -0.40535831, ..., -0.67065391,\n",
       "        -1.04594393,  0.07797893],\n",
       "       [-1.35825331, -1.51789368, -1.51819928, ...,  0.17279117,\n",
       "        -0.10906991, -0.48454426],\n",
       "       [ 1.5057434 ,  1.34508393,  1.50236906, ..., -1.32666675,\n",
       "        -1.04594393, -0.57829812],\n",
       "       ...,\n",
       "       [ 0.3919669 ,  0.54981237,  0.70748265, ...,  0.26650729,\n",
       "        -0.20275731,  1.20302531],\n",
       "       [-1.8355861 , -1.8360023 , -1.83615384, ..., -1.04551839,\n",
       "        -1.13963133, -1.14082131],\n",
       "       [-1.35825331, -1.35883937, -1.359222  , ...,  2.98427476,\n",
       "         3.26367657,  1.76554849]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 处理数据,主要是testing data\n",
    "# 此时的x不需要再用np.concatenate函数来拼接一个常数项的feature，no need:x = np.concatenate((np.ones([12 * 471, 1]), x), axis = 1).astype(float)\n",
    "\n",
    "testdata = pd.read_csv('../data/test.csv', header = None, encoding = 'big5')\n",
    "test_data = testdata.iloc[:, 2:]\n",
    "test_data[test_data == 'NR'] = 0\n",
    "test_data = test_data.to_numpy()\n",
    "test_x = np.empty([240, 18*9], dtype = float)\n",
    "for i in range(240):\n",
    "    test_x[i, :] = test_data[18 * i: 18* (i + 1), :].reshape(1, -1)\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(test_x[0])):\n",
    "        if std_x[j] != 0:\n",
    "            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]\n",
    "# no need: test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                8150      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 10,751\n",
      "Trainable params: 10,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gehao/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "4521/4521 [==============================] - 1s 181us/step - loss: 632.2474\n",
      "Epoch 2/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 400.4812\n",
      "Epoch 3/200\n",
      "4521/4521 [==============================] - 0s 31us/step - loss: 223.3624\n",
      "Epoch 4/200\n",
      "4521/4521 [==============================] - 0s 32us/step - loss: 192.3488\n",
      "Epoch 5/200\n",
      "4521/4521 [==============================] - 0s 33us/step - loss: 165.2681\n",
      "Epoch 6/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 154.4710\n",
      "Epoch 7/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 142.9358\n",
      "Epoch 8/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 142.8117\n",
      "Epoch 9/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 125.5545\n",
      "Epoch 10/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 116.7937\n",
      "Epoch 11/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 114.8604\n",
      "Epoch 12/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 115.1828\n",
      "Epoch 13/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 110.0320\n",
      "Epoch 14/200\n",
      "4521/4521 [==============================] - 0s 35us/step - loss: 103.2463\n",
      "Epoch 15/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 100.5120\n",
      "Epoch 16/200\n",
      "4521/4521 [==============================] - 0s 32us/step - loss: 100.2672\n",
      "Epoch 17/200\n",
      "4521/4521 [==============================] - 0s 32us/step - loss: 103.4789\n",
      "Epoch 18/200\n",
      "4521/4521 [==============================] - 0s 31us/step - loss: 94.8443\n",
      "Epoch 19/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 88.4283\n",
      "Epoch 20/200\n",
      "4521/4521 [==============================] - 0s 40us/step - loss: 91.9498\n",
      "Epoch 21/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 88.6363\n",
      "Epoch 22/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 93.0805\n",
      "Epoch 23/200\n",
      "4521/4521 [==============================] - 0s 35us/step - loss: 89.5808\n",
      "Epoch 24/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 88.7951\n",
      "Epoch 25/200\n",
      "4521/4521 [==============================] - 0s 24us/step - loss: 90.3701\n",
      "Epoch 26/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 86.2489\n",
      "Epoch 27/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 83.0605\n",
      "Epoch 28/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 79.2244\n",
      "Epoch 29/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 82.3951\n",
      "Epoch 30/200\n",
      "4521/4521 [==============================] - 0s 23us/step - loss: 78.7974\n",
      "Epoch 31/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 83.7269\n",
      "Epoch 32/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 79.2854\n",
      "Epoch 33/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 78.9051\n",
      "Epoch 34/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 79.4224\n",
      "Epoch 35/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 80.4480\n",
      "Epoch 36/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 81.7145\n",
      "Epoch 37/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 77.9467\n",
      "Epoch 38/200\n",
      "4521/4521 [==============================] - 0s 31us/step - loss: 77.0311\n",
      "Epoch 39/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 77.5525\n",
      "Epoch 40/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 74.2391\n",
      "Epoch 41/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 78.6270\n",
      "Epoch 42/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 74.0407\n",
      "Epoch 43/200\n",
      "4521/4521 [==============================] - 0s 22us/step - loss: 72.8445\n",
      "Epoch 44/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 74.6301\n",
      "Epoch 45/200\n",
      "4521/4521 [==============================] - 0s 31us/step - loss: 76.4873\n",
      "Epoch 46/200\n",
      "4521/4521 [==============================] - 0s 43us/step - loss: 72.9519\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4521/4521 [==============================] - 0s 32us/step - loss: 67.2772\n",
      "Epoch 48/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 71.1994\n",
      "Epoch 49/200\n",
      "4521/4521 [==============================] - 0s 34us/step - loss: 74.1816\n",
      "Epoch 50/200\n",
      "4521/4521 [==============================] - 0s 23us/step - loss: 69.4127\n",
      "Epoch 51/200\n",
      "4521/4521 [==============================] - 0s 21us/step - loss: 71.8296\n",
      "Epoch 52/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 70.8702\n",
      "Epoch 53/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 70.7594\n",
      "Epoch 54/200\n",
      "4521/4521 [==============================] - 0s 21us/step - loss: 68.4501\n",
      "Epoch 55/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 68.7566\n",
      "Epoch 56/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 65.7607\n",
      "Epoch 57/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 69.1794\n",
      "Epoch 58/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 70.2531\n",
      "Epoch 59/200\n",
      "4521/4521 [==============================] - 0s 23us/step - loss: 70.3950\n",
      "Epoch 60/200\n",
      "4521/4521 [==============================] - 0s 35us/step - loss: 68.8913\n",
      "Epoch 61/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 68.4223\n",
      "Epoch 62/200\n",
      "4521/4521 [==============================] - 0s 23us/step - loss: 66.5979\n",
      "Epoch 63/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 67.9747\n",
      "Epoch 64/200\n",
      "4521/4521 [==============================] - 0s 36us/step - loss: 66.2166\n",
      "Epoch 65/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 67.4961\n",
      "Epoch 66/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 64.4166\n",
      "Epoch 67/200\n",
      "4521/4521 [==============================] - 0s 35us/step - loss: 67.5747\n",
      "Epoch 68/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 65.5361\n",
      "Epoch 69/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 64.8760\n",
      "Epoch 70/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 64.9955\n",
      "Epoch 71/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 63.7575\n",
      "Epoch 72/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 62.8028\n",
      "Epoch 73/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 64.8579\n",
      "Epoch 74/200\n",
      "4521/4521 [==============================] - 0s 23us/step - loss: 65.3606\n",
      "Epoch 75/200\n",
      "4521/4521 [==============================] - 0s 22us/step - loss: 63.3383\n",
      "Epoch 76/200\n",
      "4521/4521 [==============================] - 0s 22us/step - loss: 63.8318\n",
      "Epoch 77/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 66.0050\n",
      "Epoch 78/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 63.0184\n",
      "Epoch 79/200\n",
      "4521/4521 [==============================] - 0s 22us/step - loss: 62.5206\n",
      "Epoch 80/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 63.0861\n",
      "Epoch 81/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 60.2696\n",
      "Epoch 82/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 64.6907\n",
      "Epoch 83/200\n",
      "4521/4521 [==============================] - 0s 34us/step - loss: 63.7711\n",
      "Epoch 84/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 60.9947\n",
      "Epoch 85/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 62.6307\n",
      "Epoch 86/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 61.5111\n",
      "Epoch 87/200\n",
      "4521/4521 [==============================] - 0s 31us/step - loss: 59.0310\n",
      "Epoch 88/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 59.4542\n",
      "Epoch 89/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 60.1866\n",
      "Epoch 90/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 61.3170\n",
      "Epoch 91/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 64.8650\n",
      "Epoch 92/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 60.8039\n",
      "Epoch 93/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 61.7585\n",
      "Epoch 94/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 62.8583\n",
      "Epoch 95/200\n",
      "4521/4521 [==============================] - 0s 33us/step - loss: 62.0237\n",
      "Epoch 96/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 58.6605\n",
      "Epoch 97/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 61.1395\n",
      "Epoch 98/200\n",
      "4521/4521 [==============================] - 0s 36us/step - loss: 58.5369\n",
      "Epoch 99/200\n",
      "4521/4521 [==============================] - 0s 32us/step - loss: 59.7282\n",
      "Epoch 100/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 60.2263\n",
      "Epoch 101/200\n",
      "4521/4521 [==============================] - 0s 22us/step - loss: 61.4157\n",
      "Epoch 102/200\n",
      "4521/4521 [==============================] - 0s 24us/step - loss: 57.8658\n",
      "Epoch 103/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 60.0656\n",
      "Epoch 104/200\n",
      "4521/4521 [==============================] - 0s 24us/step - loss: 59.0408\n",
      "Epoch 105/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 60.7468\n",
      "Epoch 106/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 60.4799\n",
      "Epoch 107/200\n",
      "4521/4521 [==============================] - 0s 34us/step - loss: 59.5060\n",
      "Epoch 108/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 59.0643\n",
      "Epoch 109/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 59.3065\n",
      "Epoch 110/200\n",
      "4521/4521 [==============================] - 0s 36us/step - loss: 58.5127\n",
      "Epoch 111/200\n",
      "4521/4521 [==============================] - 0s 32us/step - loss: 57.7884\n",
      "Epoch 112/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 55.7805\n",
      "Epoch 113/200\n",
      "4521/4521 [==============================] - 0s 21us/step - loss: 57.1231\n",
      "Epoch 114/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 57.4877\n",
      "Epoch 115/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 57.1731\n",
      "Epoch 116/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 56.1311\n",
      "Epoch 117/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 57.1686\n",
      "Epoch 118/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 55.5251\n",
      "Epoch 119/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 56.1618\n",
      "Epoch 120/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 55.5876\n",
      "Epoch 121/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 57.0191\n",
      "Epoch 122/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 54.9903\n",
      "Epoch 123/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 58.5483\n",
      "Epoch 124/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 56.1092\n",
      "Epoch 125/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 56.3994\n",
      "Epoch 126/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 56.3804\n",
      "Epoch 127/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 57.4930\n",
      "Epoch 128/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 58.9743\n",
      "Epoch 129/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 57.9527\n",
      "Epoch 130/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 54.5686\n",
      "Epoch 131/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 55.4434\n",
      "Epoch 132/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 58.2164\n",
      "Epoch 133/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 58.2147\n",
      "Epoch 134/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 55.5486\n",
      "Epoch 135/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 56.6160\n",
      "Epoch 136/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 57.9228\n",
      "Epoch 137/200\n",
      "4521/4521 [==============================] - 0s 32us/step - loss: 55.5010\n",
      "Epoch 138/200\n",
      "4521/4521 [==============================] - 0s 36us/step - loss: 56.8401\n",
      "Epoch 139/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 54.4907\n",
      "Epoch 140/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4521/4521 [==============================] - 0s 28us/step - loss: 56.2150\n",
      "Epoch 141/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 55.2262\n",
      "Epoch 142/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 55.5349\n",
      "Epoch 143/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 55.7037\n",
      "Epoch 144/200\n",
      "4521/4521 [==============================] - 0s 24us/step - loss: 55.9903\n",
      "Epoch 145/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 53.2281\n",
      "Epoch 146/200\n",
      "4521/4521 [==============================] - 0s 23us/step - loss: 52.6101\n",
      "Epoch 147/200\n",
      "4521/4521 [==============================] - 0s 21us/step - loss: 54.1180\n",
      "Epoch 148/200\n",
      "4521/4521 [==============================] - 0s 24us/step - loss: 54.8039\n",
      "Epoch 149/200\n",
      "4521/4521 [==============================] - 0s 22us/step - loss: 52.4696\n",
      "Epoch 150/200\n",
      "4521/4521 [==============================] - 0s 22us/step - loss: 54.0612\n",
      "Epoch 151/200\n",
      "4521/4521 [==============================] - 0s 32us/step - loss: 55.0647\n",
      "Epoch 152/200\n",
      "4521/4521 [==============================] - 0s 23us/step - loss: 52.6593\n",
      "Epoch 153/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 52.1393\n",
      "Epoch 154/200\n",
      "4521/4521 [==============================] - 0s 22us/step - loss: 52.6598\n",
      "Epoch 155/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 53.3316\n",
      "Epoch 156/200\n",
      "4521/4521 [==============================] - 0s 22us/step - loss: 53.0936\n",
      "Epoch 157/200\n",
      "4521/4521 [==============================] - 0s 23us/step - loss: 50.8211\n",
      "Epoch 158/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 54.1043\n",
      "Epoch 159/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 52.8458\n",
      "Epoch 160/200\n",
      "4521/4521 [==============================] - 0s 24us/step - loss: 53.7360\n",
      "Epoch 161/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 52.4483\n",
      "Epoch 162/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 52.8239\n",
      "Epoch 163/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 54.2311\n",
      "Epoch 164/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 55.0864\n",
      "Epoch 165/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 53.6555\n",
      "Epoch 166/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 52.9165\n",
      "Epoch 167/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 52.9568\n",
      "Epoch 168/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 52.6728\n",
      "Epoch 169/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 53.5851\n",
      "Epoch 170/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 51.1519\n",
      "Epoch 171/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 51.6250\n",
      "Epoch 172/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 52.2433\n",
      "Epoch 173/200\n",
      "4521/4521 [==============================] - 0s 24us/step - loss: 51.9223\n",
      "Epoch 174/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 53.5009\n",
      "Epoch 175/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 52.7763\n",
      "Epoch 176/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 51.2622\n",
      "Epoch 177/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 52.1173\n",
      "Epoch 178/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 51.8982\n",
      "Epoch 179/200\n",
      "4521/4521 [==============================] - 0s 31us/step - loss: 51.7062\n",
      "Epoch 180/200\n",
      "4521/4521 [==============================] - 0s 27us/step - loss: 53.9765\n",
      "Epoch 181/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 52.3425\n",
      "Epoch 182/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 52.8468\n",
      "Epoch 183/200\n",
      "4521/4521 [==============================] - 0s 23us/step - loss: 51.4435\n",
      "Epoch 184/200\n",
      "4521/4521 [==============================] - 0s 38us/step - loss: 50.8078\n",
      "Epoch 185/200\n",
      "4521/4521 [==============================] - 0s 35us/step - loss: 50.0995\n",
      "Epoch 186/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 49.3774\n",
      "Epoch 187/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 51.7189\n",
      "Epoch 188/200\n",
      "4521/4521 [==============================] - 0s 30us/step - loss: 50.6722\n",
      "Epoch 189/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 49.8321\n",
      "Epoch 190/200\n",
      "4521/4521 [==============================] - 0s 24us/step - loss: 49.3314\n",
      "Epoch 191/200\n",
      "4521/4521 [==============================] - 0s 40us/step - loss: 49.9626\n",
      "Epoch 192/200\n",
      "4521/4521 [==============================] - 0s 28us/step - loss: 50.7425\n",
      "Epoch 193/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 48.7786\n",
      "Epoch 194/200\n",
      "4521/4521 [==============================] - 0s 26us/step - loss: 51.3824\n",
      "Epoch 195/200\n",
      "4521/4521 [==============================] - 0s 36us/step - loss: 49.3837\n",
      "Epoch 196/200\n",
      "4521/4521 [==============================] - 0s 29us/step - loss: 47.4126\n",
      "Epoch 197/200\n",
      "4521/4521 [==============================] - 0s 32us/step - loss: 50.6143\n",
      "Epoch 198/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 51.0691\n",
      "Epoch 199/200\n",
      "4521/4521 [==============================] - 0s 31us/step - loss: 51.8671\n",
      "Epoch 200/200\n",
      "4521/4521 [==============================] - 0s 25us/step - loss: 53.5540\n",
      "4521/4521 [==============================] - 0s 40us/step\n",
      "training_set rmse: 5.079798\n",
      "validation_set rmse: 6.117177\n"
     ]
    }
   ],
   "source": [
    "# 用Keras搭建Regression的神经网络\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x_train = x_train_set\n",
    "y_train = y_train_set\n",
    "# DNN 100 5\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim = len(x_train[0]), units = 50, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1))\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(x_train, y_train, batch_size = 100, epochs = 200)\n",
    "result_train = model.evaluate(x_train, y_train)\n",
    "result_train\n",
    "\n",
    "rmse_train = np.sqrt(np.sum(np.power(model.predict(x_train)-y_train, 2)) / (len(y_train)))\n",
    "print('training_set rmse: %6f'%rmse_train)\n",
    "predict_y_validation_set = model.predict(x_validation_set)\n",
    "rmse = np.sqrt(np.sum(np.power(predict_y_validation_set - y_validation_set, 2)) / (len(y_validation_set)))\n",
    "print('validation_set rmse: %6f'%rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存model\n",
    "# model.save('regression_keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 载入保存的model，测试得到的结果与之前训练的是否相同\n",
    "# from keras.models import load_model\n",
    "# model1=load_model('regression_keras.h5')\n",
    "\n",
    "# rmse_train = np.sqrt(np.sum(np.power(model1.predict(x_train)-y_train, 2)) / (len(y_train)))\n",
    "# print('training_set rmse: %6f'%rmse_train)\n",
    "# predict_y_validation_set = model1.predict(x_validation_set)\n",
    "# rmse = np.sqrt(np.sum(np.power(predict_y_validation_set - y_validation_set, 2)) / (len(y_validation_set)))\n",
    "# print('validation_set rmse: %6f'%rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 50)                8150      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 10,751\n",
      "Trainable params: 10,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5652/5652 [==============================] - 1s 138us/step - loss: 635.3017\n",
      "Epoch 2/200\n",
      "5652/5652 [==============================] - 0s 33us/step - loss: 367.2043\n",
      "Epoch 3/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 217.4545\n",
      "Epoch 4/200\n",
      "5652/5652 [==============================] - 0s 22us/step - loss: 182.6676\n",
      "Epoch 5/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 160.9745\n",
      "Epoch 6/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 144.2442\n",
      "Epoch 7/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 138.7954\n",
      "Epoch 8/200\n",
      "5652/5652 [==============================] - 0s 31us/step - loss: 129.4814\n",
      "Epoch 9/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 118.7022\n",
      "Epoch 10/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 117.6765\n",
      "Epoch 11/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 112.0637\n",
      "Epoch 12/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 105.6346\n",
      "Epoch 13/200\n",
      "5652/5652 [==============================] - 0s 33us/step - loss: 102.5522\n",
      "Epoch 14/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 98.9154\n",
      "Epoch 15/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 96.7713\n",
      "Epoch 16/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 95.4961\n",
      "Epoch 17/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 92.5778\n",
      "Epoch 18/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 91.4864\n",
      "Epoch 19/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 87.9148\n",
      "Epoch 20/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 90.7482\n",
      "Epoch 21/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 90.4903\n",
      "Epoch 22/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 87.8453\n",
      "Epoch 23/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 83.8553\n",
      "Epoch 24/200\n",
      "5652/5652 [==============================] - 0s 32us/step - loss: 80.8108\n",
      "Epoch 25/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 84.0477\n",
      "Epoch 26/200\n",
      "5652/5652 [==============================] - 0s 34us/step - loss: 80.1784\n",
      "Epoch 27/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 78.4796\n",
      "Epoch 28/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 76.7109\n",
      "Epoch 29/200\n",
      "5652/5652 [==============================] - 0s 32us/step - loss: 77.4264\n",
      "Epoch 30/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 77.2191\n",
      "Epoch 31/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 76.1826\n",
      "Epoch 32/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 74.0105\n",
      "Epoch 33/200\n",
      "5652/5652 [==============================] - 0s 32us/step - loss: 78.7733\n",
      "Epoch 34/200\n",
      "5652/5652 [==============================] - 0s 30us/step - loss: 76.0543\n",
      "Epoch 35/200\n",
      "5652/5652 [==============================] - 0s 30us/step - loss: 75.8806\n",
      "Epoch 36/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 74.0602\n",
      "Epoch 37/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 74.1647\n",
      "Epoch 38/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 71.6895\n",
      "Epoch 39/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 72.2906\n",
      "Epoch 40/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 75.1966\n",
      "Epoch 41/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 73.3894\n",
      "Epoch 42/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 69.3598\n",
      "Epoch 43/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 70.7539\n",
      "Epoch 44/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 70.3202\n",
      "Epoch 45/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 73.0802\n",
      "Epoch 46/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 70.1187\n",
      "Epoch 47/200\n",
      "5652/5652 [==============================] - 0s 32us/step - loss: 74.0490\n",
      "Epoch 48/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 68.8151\n",
      "Epoch 49/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 67.0263\n",
      "Epoch 50/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 71.7494\n",
      "Epoch 51/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 67.4949\n",
      "Epoch 52/200\n",
      "5652/5652 [==============================] - 0s 33us/step - loss: 67.0392\n",
      "Epoch 53/200\n",
      "5652/5652 [==============================] - 0s 31us/step - loss: 68.6925\n",
      "Epoch 54/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 66.0505\n",
      "Epoch 55/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 65.4986\n",
      "Epoch 56/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 64.9523\n",
      "Epoch 57/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 65.7673\n",
      "Epoch 58/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 66.6421\n",
      "Epoch 59/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 66.2012\n",
      "Epoch 60/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 67.8816\n",
      "Epoch 61/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 66.5882\n",
      "Epoch 62/200\n",
      "5652/5652 [==============================] - 0s 35us/step - loss: 64.1528\n",
      "Epoch 63/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 65.1854\n",
      "Epoch 64/200\n",
      "5652/5652 [==============================] - 0s 30us/step - loss: 66.8920\n",
      "Epoch 65/200\n",
      "5652/5652 [==============================] - 0s 34us/step - loss: 64.0565\n",
      "Epoch 66/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 63.3337\n",
      "Epoch 67/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 63.4158\n",
      "Epoch 68/200\n",
      "5652/5652 [==============================] - 0s 30us/step - loss: 61.0841\n",
      "Epoch 69/200\n",
      "5652/5652 [==============================] - 0s 32us/step - loss: 60.6562\n",
      "Epoch 70/200\n",
      "5652/5652 [==============================] - 0s 33us/step - loss: 61.4720\n",
      "Epoch 71/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 61.4846\n",
      "Epoch 72/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 63.8187\n",
      "Epoch 73/200\n",
      "5652/5652 [==============================] - 0s 34us/step - loss: 62.7893\n",
      "Epoch 74/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 59.1455\n",
      "Epoch 75/200\n",
      "5652/5652 [==============================] - 0s 33us/step - loss: 60.9469\n",
      "Epoch 76/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 61.1424\n",
      "Epoch 77/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 60.1553\n",
      "Epoch 78/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 59.5407\n",
      "Epoch 79/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 60.5877\n",
      "Epoch 80/200\n",
      "5652/5652 [==============================] - 0s 30us/step - loss: 60.9443\n",
      "Epoch 81/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 59.1452\n",
      "Epoch 82/200\n",
      "5652/5652 [==============================] - 0s 37us/step - loss: 60.2320\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5652/5652 [==============================] - 0s 31us/step - loss: 57.8015\n",
      "Epoch 84/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 62.0804\n",
      "Epoch 85/200\n",
      "5652/5652 [==============================] - 0s 35us/step - loss: 60.3918\n",
      "Epoch 86/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 58.4406\n",
      "Epoch 87/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 59.9545\n",
      "Epoch 88/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 56.3975\n",
      "Epoch 89/200\n",
      "5652/5652 [==============================] - 0s 22us/step - loss: 59.9143\n",
      "Epoch 90/200\n",
      "5652/5652 [==============================] - 0s 22us/step - loss: 56.7839\n",
      "Epoch 91/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 57.0518\n",
      "Epoch 92/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 58.0610\n",
      "Epoch 93/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 58.8821\n",
      "Epoch 94/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 58.1220\n",
      "Epoch 95/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 57.3231\n",
      "Epoch 96/200\n",
      "5652/5652 [==============================] - 0s 22us/step - loss: 56.4756\n",
      "Epoch 97/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 55.7772\n",
      "Epoch 98/200\n",
      "5652/5652 [==============================] - 0s 22us/step - loss: 57.1927\n",
      "Epoch 99/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 59.4981\n",
      "Epoch 100/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 56.4321\n",
      "Epoch 101/200\n",
      "5652/5652 [==============================] - 0s 22us/step - loss: 58.1121\n",
      "Epoch 102/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 59.0472\n",
      "Epoch 103/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 57.0137\n",
      "Epoch 104/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 56.0037\n",
      "Epoch 105/200\n",
      "5652/5652 [==============================] - 0s 21us/step - loss: 55.4519\n",
      "Epoch 106/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 54.9707\n",
      "Epoch 107/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 56.6166\n",
      "Epoch 108/200\n",
      "5652/5652 [==============================] - 0s 31us/step - loss: 54.4587\n",
      "Epoch 109/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 56.9800\n",
      "Epoch 110/200\n",
      "5652/5652 [==============================] - 0s 21us/step - loss: 55.9411\n",
      "Epoch 111/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 55.9894\n",
      "Epoch 112/200\n",
      "5652/5652 [==============================] - 0s 35us/step - loss: 56.1806\n",
      "Epoch 113/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 54.2160\n",
      "Epoch 114/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 52.4333\n",
      "Epoch 115/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 55.5555\n",
      "Epoch 116/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 55.7854\n",
      "Epoch 117/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 54.4353\n",
      "Epoch 118/200\n",
      "5652/5652 [==============================] - 0s 22us/step - loss: 52.9181\n",
      "Epoch 119/200\n",
      "5652/5652 [==============================] - 0s 36us/step - loss: 55.8792\n",
      "Epoch 120/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 53.4275\n",
      "Epoch 121/200\n",
      "5652/5652 [==============================] - 0s 31us/step - loss: 53.2810\n",
      "Epoch 122/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 53.5528\n",
      "Epoch 123/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 53.5962\n",
      "Epoch 124/200\n",
      "5652/5652 [==============================] - 0s 36us/step - loss: 53.4993\n",
      "Epoch 125/200\n",
      "5652/5652 [==============================] - 0s 32us/step - loss: 53.0840\n",
      "Epoch 126/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 54.0941\n",
      "Epoch 127/200\n",
      "5652/5652 [==============================] - 0s 34us/step - loss: 53.1640\n",
      "Epoch 128/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 54.4907\n",
      "Epoch 129/200\n",
      "5652/5652 [==============================] - 0s 33us/step - loss: 54.1643\n",
      "Epoch 130/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 53.0840\n",
      "Epoch 131/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 54.0560\n",
      "Epoch 132/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 53.2695\n",
      "Epoch 133/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 53.3708\n",
      "Epoch 134/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 52.2964\n",
      "Epoch 135/200\n",
      "5652/5652 [==============================] - 0s 30us/step - loss: 54.2724\n",
      "Epoch 136/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 54.5924\n",
      "Epoch 137/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 50.8196\n",
      "Epoch 138/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 53.9551\n",
      "Epoch 139/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 52.1984\n",
      "Epoch 140/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 51.2929\n",
      "Epoch 141/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 51.3415\n",
      "Epoch 142/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 54.1607\n",
      "Epoch 143/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 52.3234\n",
      "Epoch 144/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 51.8396\n",
      "Epoch 145/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 51.1918\n",
      "Epoch 146/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 51.4890\n",
      "Epoch 147/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 53.5254\n",
      "Epoch 148/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 51.6106\n",
      "Epoch 149/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 52.4093\n",
      "Epoch 150/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 51.2959\n",
      "Epoch 151/200\n",
      "5652/5652 [==============================] - 0s 33us/step - loss: 50.7004\n",
      "Epoch 152/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 50.4699\n",
      "Epoch 153/200\n",
      "5652/5652 [==============================] - 0s 43us/step - loss: 51.1262\n",
      "Epoch 154/200\n",
      "5652/5652 [==============================] - 0s 35us/step - loss: 52.3315\n",
      "Epoch 155/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 50.0458\n",
      "Epoch 156/200\n",
      "5652/5652 [==============================] - 0s 34us/step - loss: 50.0906\n",
      "Epoch 157/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 51.0741\n",
      "Epoch 158/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 51.9501\n",
      "Epoch 159/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 49.6125\n",
      "Epoch 160/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 50.3934\n",
      "Epoch 161/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 49.5184\n",
      "Epoch 162/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 51.9052\n",
      "Epoch 163/200\n",
      "5652/5652 [==============================] - 0s 37us/step - loss: 49.6304\n",
      "Epoch 164/200\n",
      "5652/5652 [==============================] - 0s 30us/step - loss: 50.4033\n",
      "Epoch 165/200\n",
      "5652/5652 [==============================] - 0s 28us/step - loss: 50.5771\n",
      "Epoch 166/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 50.4015\n",
      "Epoch 167/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 52.5410\n",
      "Epoch 168/200\n",
      "5652/5652 [==============================] - 0s 25us/step - loss: 48.9163\n",
      "Epoch 169/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 50.3340\n",
      "Epoch 170/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 49.9322\n",
      "Epoch 171/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 51.4859\n",
      "Epoch 172/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 49.7805\n",
      "Epoch 173/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 48.3119\n",
      "Epoch 174/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 48.8546\n",
      "Epoch 175/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 49.8962\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5652/5652 [==============================] - 0s 26us/step - loss: 48.1195\n",
      "Epoch 177/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 49.2183\n",
      "Epoch 178/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 49.5364\n",
      "Epoch 179/200\n",
      "5652/5652 [==============================] - 0s 21us/step - loss: 49.1966\n",
      "Epoch 180/200\n",
      "5652/5652 [==============================] - 0s 33us/step - loss: 48.3526\n",
      "Epoch 181/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 51.4624\n",
      "Epoch 182/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 48.8570\n",
      "Epoch 183/200\n",
      "5652/5652 [==============================] - 0s 22us/step - loss: 49.3992\n",
      "Epoch 184/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 49.2802\n",
      "Epoch 185/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 46.8807\n",
      "Epoch 186/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 48.2820\n",
      "Epoch 187/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 49.6444\n",
      "Epoch 188/200\n",
      "5652/5652 [==============================] - 0s 23us/step - loss: 48.1531\n",
      "Epoch 189/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 49.9807\n",
      "Epoch 190/200\n",
      "5652/5652 [==============================] - 0s 30us/step - loss: 48.7238\n",
      "Epoch 191/200\n",
      "5652/5652 [==============================] - 0s 24us/step - loss: 50.5106\n",
      "Epoch 192/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 49.1447\n",
      "Epoch 193/200\n",
      "5652/5652 [==============================] - 0s 34us/step - loss: 48.4988\n",
      "Epoch 194/200\n",
      "5652/5652 [==============================] - 0s 29us/step - loss: 48.2261\n",
      "Epoch 195/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 48.7491\n",
      "Epoch 196/200\n",
      "5652/5652 [==============================] - 0s 22us/step - loss: 50.4564\n",
      "Epoch 197/200\n",
      "5652/5652 [==============================] - 0s 32us/step - loss: 50.9877\n",
      "Epoch 198/200\n",
      "5652/5652 [==============================] - 0s 32us/step - loss: 46.4605\n",
      "Epoch 199/200\n",
      "5652/5652 [==============================] - 0s 26us/step - loss: 49.6525\n",
      "Epoch 200/200\n",
      "5652/5652 [==============================] - 0s 27us/step - loss: 48.2573\n",
      "5652/5652 [==============================] - 0s 34us/step\n",
      "training_all rmse: 5.148199\n"
     ]
    }
   ],
   "source": [
    "# 用全部的data对model进行训练\n",
    "x_train = x\n",
    "y_train = y\n",
    "# DNN \n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim = len(x_train[0]), units = 50, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1))\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(x_train, y_train, batch_size = 100, epochs = 200)\n",
    "result_train = model.evaluate(x_train, y_train)\n",
    "result_train\n",
    "\n",
    "rmse_train = np.sqrt(np.sum(np.power(model.predict(x_train)-y_train, 2)) / (len(y_train)))\n",
    "print('training_all rmse: %6f'%rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存用全部data训练好的model\n",
    "model.save('regression_keras_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_all rmse: 5.148199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.580561 ],\n",
       "       [14.931612 ],\n",
       "       [23.954575 ],\n",
       "       [ 6.94833  ],\n",
       "       [26.589321 ],\n",
       "       [20.832436 ],\n",
       "       [20.848267 ],\n",
       "       [28.117191 ],\n",
       "       [16.325716 ],\n",
       "       [57.77251  ],\n",
       "       [10.829015 ],\n",
       "       [ 8.295121 ],\n",
       "       [58.00958  ],\n",
       "       [54.287766 ],\n",
       "       [21.789528 ],\n",
       "       [10.860449 ],\n",
       "       [31.443542 ],\n",
       "       [63.541145 ],\n",
       "       [ 3.4695394],\n",
       "       [15.793261 ],\n",
       "       [44.32576  ],\n",
       "       [65.814835 ],\n",
       "       [ 8.346945 ],\n",
       "       [16.764254 ],\n",
       "       [13.020544 ],\n",
       "       [36.118088 ],\n",
       "       [13.349997 ],\n",
       "       [62.38421  ],\n",
       "       [ 6.850695 ],\n",
       "       [51.911156 ],\n",
       "       [21.450102 ],\n",
       "       [ 7.9144974],\n",
       "       [ 5.2193966],\n",
       "       [17.683403 ],\n",
       "       [26.594868 ],\n",
       "       [37.979584 ],\n",
       "       [47.62962  ],\n",
       "       [30.029968 ],\n",
       "       [46.768658 ],\n",
       "       [32.364414 ],\n",
       "       [ 6.841839 ],\n",
       "       [45.903664 ],\n",
       "       [27.732224 ],\n",
       "       [51.665257 ],\n",
       "       [14.896238 ],\n",
       "       [36.558475 ],\n",
       "       [23.986755 ],\n",
       "       [ 9.296665 ],\n",
       "       [24.48132  ],\n",
       "       [27.785278 ],\n",
       "       [20.35164  ],\n",
       "       [ 6.5610557],\n",
       "       [17.553682 ],\n",
       "       [54.749985 ],\n",
       "       [10.864498 ],\n",
       "       [31.145916 ],\n",
       "       [33.80333  ],\n",
       "       [21.541208 ],\n",
       "       [56.348236 ],\n",
       "       [21.4034   ],\n",
       "       [12.3725605],\n",
       "       [38.69373  ],\n",
       "       [10.497805 ],\n",
       "       [52.881004 ],\n",
       "       [10.966631 ],\n",
       "       [11.989462 ],\n",
       "       [13.292728 ],\n",
       "       [ 3.4681892],\n",
       "       [41.949158 ],\n",
       "       [28.322947 ],\n",
       "       [15.543697 ],\n",
       "       [42.934334 ],\n",
       "       [61.075684 ],\n",
       "       [ 4.730114 ],\n",
       "       [16.587711 ],\n",
       "       [ 5.4637165],\n",
       "       [41.63206  ],\n",
       "       [11.6627655],\n",
       "       [22.222961 ],\n",
       "       [21.717415 ],\n",
       "       [23.128714 ],\n",
       "       [32.62682  ],\n",
       "       [20.728516 ],\n",
       "       [73.31369  ],\n",
       "       [39.763813 ],\n",
       "       [26.729618 ],\n",
       "       [20.026878 ],\n",
       "       [33.289505 ],\n",
       "       [22.099165 ],\n",
       "       [19.894201 ],\n",
       "       [30.180145 ],\n",
       "       [41.60785  ],\n",
       "       [ 7.1591787],\n",
       "       [38.098824 ],\n",
       "       [47.95078  ],\n",
       "       [13.413099 ],\n",
       "       [32.961723 ],\n",
       "       [12.813713 ],\n",
       "       [20.024347 ],\n",
       "       [ 4.7608457],\n",
       "       [16.175106 ],\n",
       "       [23.593918 ],\n",
       "       [13.905365 ],\n",
       "       [13.617882 ],\n",
       "       [22.038185 ],\n",
       "       [36.736008 ],\n",
       "       [32.261368 ],\n",
       "       [ 7.091723 ],\n",
       "       [ 4.541971 ],\n",
       "       [68.98833  ],\n",
       "       [50.109463 ],\n",
       "       [13.142601 ],\n",
       "       [25.735859 ],\n",
       "       [16.711353 ],\n",
       "       [11.490347 ],\n",
       "       [24.344704 ],\n",
       "       [25.328587 ],\n",
       "       [ 7.9157357],\n",
       "       [18.094    ],\n",
       "       [17.909077 ],\n",
       "       [69.16941  ],\n",
       "       [26.543552 ],\n",
       "       [38.13735  ],\n",
       "       [23.150108 ],\n",
       "       [ 9.080875 ],\n",
       "       [40.99392  ],\n",
       "       [ 9.535878 ],\n",
       "       [21.732399 ],\n",
       "       [26.697641 ],\n",
       "       [61.185505 ],\n",
       "       [17.194416 ],\n",
       "       [20.67906  ],\n",
       "       [57.863777 ],\n",
       "       [12.933735 ],\n",
       "       [14.357525 ],\n",
       "       [ 4.256483 ],\n",
       "       [11.558126 ],\n",
       "       [61.023315 ],\n",
       "       [12.097723 ],\n",
       "       [ 3.6050775],\n",
       "       [26.242289 ],\n",
       "       [24.535767 ],\n",
       "       [50.919575 ],\n",
       "       [27.906052 ],\n",
       "       [15.739452 ],\n",
       "       [26.160305 ],\n",
       "       [ 7.450863 ],\n",
       "       [53.548744 ],\n",
       "       [22.255394 ],\n",
       "       [35.845005 ],\n",
       "       [ 8.585251 ],\n",
       "       [ 4.807899 ],\n",
       "       [23.204758 ],\n",
       "       [ 5.04486  ],\n",
       "       [14.759708 ],\n",
       "       [38.805412 ],\n",
       "       [ 6.3088884],\n",
       "       [37.29309  ],\n",
       "       [ 9.68781  ],\n",
       "       [15.934109 ],\n",
       "       [39.18811  ],\n",
       "       [15.288351 ],\n",
       "       [13.214277 ],\n",
       "       [ 7.5708156],\n",
       "       [55.861115 ],\n",
       "       [32.19979  ],\n",
       "       [ 3.2078094],\n",
       "       [13.359554 ],\n",
       "       [61.690517 ],\n",
       "       [12.75807  ],\n",
       "       [65.22306  ],\n",
       "       [45.425926 ],\n",
       "       [25.577217 ],\n",
       "       [18.690971 ],\n",
       "       [55.818634 ],\n",
       "       [23.776608 ],\n",
       "       [20.328695 ],\n",
       "       [41.60852  ],\n",
       "       [11.155957 ],\n",
       "       [31.382656 ],\n",
       "       [14.475452 ],\n",
       "       [ 9.2453785],\n",
       "       [60.817307 ],\n",
       "       [46.10502  ],\n",
       "       [13.22928  ],\n",
       "       [37.83105  ],\n",
       "       [24.738935 ],\n",
       "       [63.42414  ],\n",
       "       [10.753617 ],\n",
       "       [50.55554  ],\n",
       "       [39.736374 ],\n",
       "       [15.506561 ],\n",
       "       [28.203358 ],\n",
       "       [ 3.753931 ],\n",
       "       [18.321766 ],\n",
       "       [ 3.837377 ],\n",
       "       [34.396484 ],\n",
       "       [11.341259 ],\n",
       "       [20.708012 ],\n",
       "       [55.718857 ],\n",
       "       [23.475245 ],\n",
       "       [15.523867 ],\n",
       "       [60.462738 ],\n",
       "       [ 8.709477 ],\n",
       "       [ 7.4331117],\n",
       "       [ 7.6468463],\n",
       "       [ 7.8168774],\n",
       "       [ 4.003671 ],\n",
       "       [76.40396  ],\n",
       "       [19.153114 ],\n",
       "       [10.991051 ],\n",
       "       [12.536745 ],\n",
       "       [35.896385 ],\n",
       "       [36.300926 ],\n",
       "       [11.826685 ],\n",
       "       [34.888737 ],\n",
       "       [65.30974  ],\n",
       "       [ 4.128104 ],\n",
       "       [10.442388 ],\n",
       "       [33.891296 ],\n",
       "       [13.013065 ],\n",
       "       [13.730293 ],\n",
       "       [72.122696 ],\n",
       "       [12.143751 ],\n",
       "       [15.511425 ],\n",
       "       [58.220833 ],\n",
       "       [14.098837 ],\n",
       "       [18.58631  ],\n",
       "       [ 8.058299 ],\n",
       "       [ 7.486502 ],\n",
       "       [43.319374 ],\n",
       "       [10.693892 ],\n",
       "       [54.422302 ],\n",
       "       [39.410313 ],\n",
       "       [20.175764 ],\n",
       "       [42.60596  ],\n",
       "       [65.08859  ],\n",
       "       [36.625866 ],\n",
       "       [10.514151 ],\n",
       "       [14.14748  ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载model并预测\n",
    "from keras.models import load_model\n",
    "\n",
    "model_all_data = load_model('regression_keras_final.h5')\n",
    "rmse_train = np.sqrt(np.sum(np.power(model_all_data.predict(x_train)-y_train, 2)) / (len(y_train)))\n",
    "print('training_all rmse: %6f'%rmse_train)\n",
    "\n",
    "predict_y_keras = model_all_data.predict(test_x)\n",
    "predict_y_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-HVDMDOjD1st",
    "outputId": "982ea6e6-1cba-450d-d2cd-0b8443660367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'value']\n",
      "['id_0', 5.580561]\n",
      "['id_1', 14.931612]\n",
      "['id_2', 23.954575]\n",
      "['id_3', 6.94833]\n",
      "['id_4', 26.589321]\n",
      "['id_5', 20.832436]\n",
      "['id_6', 20.848267]\n",
      "['id_7', 28.117191]\n",
      "['id_8', 16.325716]\n",
      "['id_9', 57.77251]\n",
      "['id_10', 10.829015]\n",
      "['id_11', 8.295121]\n",
      "['id_12', 58.00958]\n",
      "['id_13', 54.287766]\n",
      "['id_14', 21.789528]\n",
      "['id_15', 10.860449]\n",
      "['id_16', 31.443542]\n",
      "['id_17', 63.541145]\n",
      "['id_18', 3.4695394]\n",
      "['id_19', 15.793261]\n",
      "['id_20', 44.32576]\n",
      "['id_21', 65.814835]\n",
      "['id_22', 8.346945]\n",
      "['id_23', 16.764254]\n",
      "['id_24', 13.020544]\n",
      "['id_25', 36.118088]\n",
      "['id_26', 13.349997]\n",
      "['id_27', 62.38421]\n",
      "['id_28', 6.850695]\n",
      "['id_29', 51.911156]\n",
      "['id_30', 21.450102]\n",
      "['id_31', 7.9144974]\n",
      "['id_32', 5.2193966]\n",
      "['id_33', 17.683403]\n",
      "['id_34', 26.594868]\n",
      "['id_35', 37.979584]\n",
      "['id_36', 47.62962]\n",
      "['id_37', 30.029968]\n",
      "['id_38', 46.768658]\n",
      "['id_39', 32.364414]\n",
      "['id_40', 6.841839]\n",
      "['id_41', 45.903664]\n",
      "['id_42', 27.732224]\n",
      "['id_43', 51.665257]\n",
      "['id_44', 14.896238]\n",
      "['id_45', 36.558475]\n",
      "['id_46', 23.986755]\n",
      "['id_47', 9.296665]\n",
      "['id_48', 24.48132]\n",
      "['id_49', 27.785278]\n",
      "['id_50', 20.35164]\n",
      "['id_51', 6.5610557]\n",
      "['id_52', 17.553682]\n",
      "['id_53', 54.749985]\n",
      "['id_54', 10.864498]\n",
      "['id_55', 31.145916]\n",
      "['id_56', 33.80333]\n",
      "['id_57', 21.541208]\n",
      "['id_58', 56.348236]\n",
      "['id_59', 21.4034]\n",
      "['id_60', 12.3725605]\n",
      "['id_61', 38.69373]\n",
      "['id_62', 10.497805]\n",
      "['id_63', 52.881004]\n",
      "['id_64', 10.966631]\n",
      "['id_65', 11.989462]\n",
      "['id_66', 13.292728]\n",
      "['id_67', 3.4681892]\n",
      "['id_68', 41.949158]\n",
      "['id_69', 28.322947]\n",
      "['id_70', 15.543697]\n",
      "['id_71', 42.934334]\n",
      "['id_72', 61.075684]\n",
      "['id_73', 4.730114]\n",
      "['id_74', 16.587711]\n",
      "['id_75', 5.4637165]\n",
      "['id_76', 41.63206]\n",
      "['id_77', 11.6627655]\n",
      "['id_78', 22.222961]\n",
      "['id_79', 21.717415]\n",
      "['id_80', 23.128714]\n",
      "['id_81', 32.62682]\n",
      "['id_82', 20.728516]\n",
      "['id_83', 73.31369]\n",
      "['id_84', 39.763813]\n",
      "['id_85', 26.729618]\n",
      "['id_86', 20.026878]\n",
      "['id_87', 33.289505]\n",
      "['id_88', 22.099165]\n",
      "['id_89', 19.894201]\n",
      "['id_90', 30.180145]\n",
      "['id_91', 41.60785]\n",
      "['id_92', 7.1591787]\n",
      "['id_93', 38.098824]\n",
      "['id_94', 47.95078]\n",
      "['id_95', 13.413099]\n",
      "['id_96', 32.961723]\n",
      "['id_97', 12.813713]\n",
      "['id_98', 20.024347]\n",
      "['id_99', 4.7608457]\n",
      "['id_100', 16.175106]\n",
      "['id_101', 23.593918]\n",
      "['id_102', 13.905365]\n",
      "['id_103', 13.617882]\n",
      "['id_104', 22.038185]\n",
      "['id_105', 36.736008]\n",
      "['id_106', 32.261368]\n",
      "['id_107', 7.091723]\n",
      "['id_108', 4.541971]\n",
      "['id_109', 68.98833]\n",
      "['id_110', 50.109463]\n",
      "['id_111', 13.142601]\n",
      "['id_112', 25.735859]\n",
      "['id_113', 16.711353]\n",
      "['id_114', 11.490347]\n",
      "['id_115', 24.344704]\n",
      "['id_116', 25.328587]\n",
      "['id_117', 7.9157357]\n",
      "['id_118', 18.094]\n",
      "['id_119', 17.909077]\n",
      "['id_120', 69.16941]\n",
      "['id_121', 26.543552]\n",
      "['id_122', 38.13735]\n",
      "['id_123', 23.150108]\n",
      "['id_124', 9.080875]\n",
      "['id_125', 40.99392]\n",
      "['id_126', 9.535878]\n",
      "['id_127', 21.732399]\n",
      "['id_128', 26.697641]\n",
      "['id_129', 61.185505]\n",
      "['id_130', 17.194416]\n",
      "['id_131', 20.67906]\n",
      "['id_132', 57.863777]\n",
      "['id_133', 12.933735]\n",
      "['id_134', 14.357525]\n",
      "['id_135', 4.256483]\n",
      "['id_136', 11.558126]\n",
      "['id_137', 61.023315]\n",
      "['id_138', 12.097723]\n",
      "['id_139', 3.6050775]\n",
      "['id_140', 26.242289]\n",
      "['id_141', 24.535767]\n",
      "['id_142', 50.919575]\n",
      "['id_143', 27.906052]\n",
      "['id_144', 15.739452]\n",
      "['id_145', 26.160305]\n",
      "['id_146', 7.450863]\n",
      "['id_147', 53.548744]\n",
      "['id_148', 22.255394]\n",
      "['id_149', 35.845005]\n",
      "['id_150', 8.585251]\n",
      "['id_151', 4.807899]\n",
      "['id_152', 23.204758]\n",
      "['id_153', 5.04486]\n",
      "['id_154', 14.759708]\n",
      "['id_155', 38.805412]\n",
      "['id_156', 6.3088884]\n",
      "['id_157', 37.29309]\n",
      "['id_158', 9.68781]\n",
      "['id_159', 15.934109]\n",
      "['id_160', 39.18811]\n",
      "['id_161', 15.288351]\n",
      "['id_162', 13.214277]\n",
      "['id_163', 7.5708156]\n",
      "['id_164', 55.861115]\n",
      "['id_165', 32.19979]\n",
      "['id_166', 3.2078094]\n",
      "['id_167', 13.359554]\n",
      "['id_168', 61.690517]\n",
      "['id_169', 12.75807]\n",
      "['id_170', 65.22306]\n",
      "['id_171', 45.425926]\n",
      "['id_172', 25.577217]\n",
      "['id_173', 18.690971]\n",
      "['id_174', 55.818634]\n",
      "['id_175', 23.776608]\n",
      "['id_176', 20.328695]\n",
      "['id_177', 41.60852]\n",
      "['id_178', 11.155957]\n",
      "['id_179', 31.382656]\n",
      "['id_180', 14.475452]\n",
      "['id_181', 9.2453785]\n",
      "['id_182', 60.817307]\n",
      "['id_183', 46.10502]\n",
      "['id_184', 13.22928]\n",
      "['id_185', 37.83105]\n",
      "['id_186', 24.738935]\n",
      "['id_187', 63.42414]\n",
      "['id_188', 10.753617]\n",
      "['id_189', 50.55554]\n",
      "['id_190', 39.736374]\n",
      "['id_191', 15.506561]\n",
      "['id_192', 28.203358]\n",
      "['id_193', 3.753931]\n",
      "['id_194', 18.321766]\n",
      "['id_195', 3.837377]\n",
      "['id_196', 34.396484]\n",
      "['id_197', 11.341259]\n",
      "['id_198', 20.708012]\n",
      "['id_199', 55.718857]\n",
      "['id_200', 23.475245]\n",
      "['id_201', 15.523867]\n",
      "['id_202', 60.462738]\n",
      "['id_203', 8.709477]\n",
      "['id_204', 7.4331117]\n",
      "['id_205', 7.6468463]\n",
      "['id_206', 7.8168774]\n",
      "['id_207', 4.003671]\n",
      "['id_208', 76.40396]\n",
      "['id_209', 19.153114]\n",
      "['id_210', 10.991051]\n",
      "['id_211', 12.536745]\n",
      "['id_212', 35.896385]\n",
      "['id_213', 36.300926]\n",
      "['id_214', 11.826685]\n",
      "['id_215', 34.888737]\n",
      "['id_216', 65.30974]\n",
      "['id_217', 4.128104]\n",
      "['id_218', 10.442388]\n",
      "['id_219', 33.891296]\n",
      "['id_220', 13.013065]\n",
      "['id_221', 13.730293]\n",
      "['id_222', 72.122696]\n",
      "['id_223', 12.143751]\n",
      "['id_224', 15.511425]\n",
      "['id_225', 58.220833]\n",
      "['id_226', 14.098837]\n",
      "['id_227', 18.58631]\n",
      "['id_228', 8.058299]\n",
      "['id_229', 7.486502]\n",
      "['id_230', 43.319374]\n",
      "['id_231', 10.693892]\n",
      "['id_232', 54.422302]\n",
      "['id_233', 39.410313]\n",
      "['id_234', 20.175764]\n",
      "['id_235', 42.60596]\n",
      "['id_236', 65.08859]\n",
      "['id_237', 36.625866]\n",
      "['id_238', 10.514151]\n",
      "['id_239', 14.14748]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('predict_keras_all_data.csv', mode='w', newline='') as predict_file:\n",
    "    csv_writer = csv.writer(predict_file)\n",
    "    header = ['id', 'value']\n",
    "    print(header)\n",
    "    csv_writer.writerow(header)\n",
    "    for i in range(240):\n",
    "        row = ['id_' + str(i), predict_y_keras[i][0]]\n",
    "        csv_writer.writerow(row)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zCSf7FNECFY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw1_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('ml': conda)",
   "language": "python",
   "name": "python36864bitmlconda4727b916de1c4a8f8265036fed6d2bb8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
